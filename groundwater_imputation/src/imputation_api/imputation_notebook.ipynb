{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import utils_preprocess\n",
    "import utils_spatial_interpolation\n",
    "import utils_spatial_analysis\n",
    "\n",
    "from config import Config\n",
    "from utils_satellite_imputation import satellite_imputation\n",
    "from utils_iterative_refinement import iterative_refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Load shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile\n",
    "path_shape = Config.shapefile_path\n",
    "aquifer_shape = utils.load_shapefile(path=path_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Preprocessing PDSI and GLDAS data into Tabular Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the pdsi netcdf files to obtain tabular data pickle file\n",
    "pdsi_source_directory = Config.pdsi_source_directory\n",
    "pdsi_target_directory = Config.pdsi_target_directory\n",
    "\n",
    "utils_preprocess.process_pdsi_data(\n",
    "    source_directory=pdsi_source_directory, \n",
    "    target_directory=pdsi_target_directory,\n",
    "    date_start=Config.pdsi_preprocessing_start,\n",
    "    date_end=Config.pdsi_preprocessing_end,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the gldas netcdf files to obtain tabular data pickle file\n",
    "gldas_source_directory = Config.gldas_source_directory\n",
    "gldas_target_directory = Config.gldas_target_directory\n",
    "\n",
    "utils_preprocess.process_gldas_data(\n",
    "    source_directory=gldas_source_directory,\n",
    "    target_directory=gldas_target_directory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Transform PDSI, GLDAS, and Well Observations into format for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process PDSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse pdsi data and save it\n",
    "directory_pdsi = Config.pdsi_target_directory\n",
    "\n",
    "pdsi:dict = utils.pull_relevant_data(\n",
    "    shape=aquifer_shape, \n",
    "    dataset_name=Config.pdsi_dataset_name,\n",
    "    dataset_directory=directory_pdsi\n",
    "    )\n",
    "\n",
    "utils.save_pickle(\n",
    "    data=pdsi, \n",
    "    file_name=Config.pdsi_file_name,\n",
    "    directory=Config.dataset_directory,\n",
    "    protocol=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process GLDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the GLDAS data and save it\n",
    "directory_gldas = Config.gldas_target_directory\n",
    "\n",
    "gldas:dict = utils.pull_relevant_data(\n",
    "    shape=aquifer_shape, \n",
    "    dataset_name=Config.gldas_dataset_name,\n",
    "    dataset_directory=directory_gldas\n",
    "    )\n",
    "\n",
    "utils.save_pickle(\n",
    "    data=gldas, \n",
    "    file_name=Config.gldas_file_name,\n",
    "    directory=Config.dataset_directory,\n",
    "    protocol=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Well Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process well data from csv files\n",
    "well_locations = pd.read_csv(Config.well_locations_path)\n",
    "well_timeseries = pd.read_csv(Config.well_timeseries_path)\n",
    "\n",
    "data: dict = utils.transform_well_data(\n",
    "    well_timeseries=well_timeseries,\n",
    "    well_locations=well_locations,\n",
    "    timeseries_name=Config.well_data_timeseries_name,\n",
    "    locations_name=Config.well_data_locations_name,\n",
    "    std_threshold=Config.well_max_std,\n",
    "    min_monthly_obs=Config.well_min_observations,\n",
    "    gap_size=Config.well_gap_size,\n",
    "    pad=Config.well_padding,\n",
    "    start_date=Config.well_processing_start,\n",
    "    end_date=Config.well_processing_end,\n",
    ")\n",
    "\n",
    "utils.save_pickle(\n",
    "    data=data, \n",
    "    file_name=Config.well_data_file_name,\n",
    "    directory=Config.dataset_directory,\n",
    "    protocol=3,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the timeseries data to see if it looks reasonable\n",
    "plt.plot(data[\"timeseries\"], '-.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Develop initial imputation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satellite_imputation(\n",
    "    aquifer_name=Config.aquifer_name,\n",
    "    pdsi_pickle=Config.pdsi_file_name,\n",
    "    gldas_pickle=Config.gldas_file_name,\n",
    "    well_data_pickle=Config.well_data_file_name,\n",
    "    output_file=Config.satellite_imputation_output_file,\n",
    "    timeseries_name=Config.well_data_timeseries_name,\n",
    "    locations_name=Config.well_data_locations_name,\n",
    "    validation_split=Config.validation_split,\n",
    "    folds=Config.folds,\n",
    "    batch_size=Config.batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Develop iterative refinement model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_refinement(\n",
    "    aquifer_name=Config.aquifer_name,\n",
    "    imputed_data_pickle=Config.satellite_imputation_output_file,\n",
    "    output_file=Config.iterative_imputation_output_file,\n",
    "    validation_split=Config.validation_split,\n",
    "    folds=Config.folds,\n",
    "    feature_threshold=Config.feature_threshold,\n",
    "    n_iterations=Config.n_iterations_refinement,\n",
    "    batch_size=Config.batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Analyze spatial characteristics of imputation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_spatial_interpolation.kriging_interpolation(\n",
    "    data_pickle_path=(Config.dataset_directory\n",
    "    + \"/\" + Config.spatial_input_file),\n",
    "    shape_file_path=Config.shapefile_path,\n",
    "    n_x_cells=Config.number_of_x_cells,\n",
    "    influence_distance=Config.influence_distance,\n",
    "    monthly_time_step=Config.monthly_time_step,\n",
    "    netcdf_filename=Config.raster_file_name,\n",
    "    directory=Config.dataset_directory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Calculate Storage Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster = nc.Dataset(\n",
    "        Config.dataset_directory + \"/\" + Config.raster_file_name,\n",
    "        \"r\",\n",
    "    )\n",
    "\n",
    "spatial_analysis = utils_spatial_analysis.StorageChangeCalculator(\n",
    "    units=Config.units,\n",
    "    storage_coefficient=Config.storage_coefficient,\n",
    "    anisotropic=Config.anisotropic,\n",
    "    )\n",
    "\n",
    "storage_change = spatial_analysis.calulate_storage_curve(\n",
    "    raster=raster, \n",
    "    date_range_filter=(Config.date_filter_start, Config.date_filter_end), # if you need to filter dates within of original time range\n",
    "    dataset_directory=Config.dataset_directory,\n",
    "    filename=Config.storage_change_file_name,\n",
    "    )\n",
    "\n",
    "plt.plot(storage_change, '-.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Geospatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
