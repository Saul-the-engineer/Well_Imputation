{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting iteration 1 of 2\n",
      "INFO:utils:Pickle file 'beryl_enterpris_imputation_iteration_0.pickle' loaded successfully from '/home/saul/workspace/groundwater_well_imputation/groundwater_imputation_api/src/imputation_api/artifacts/dataset_outputs'\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:Starting imputation for well: 373338113431502\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import utils\n",
    "import utils_ml\n",
    "import logging\n",
    "import traceback\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "from keras import callbacks\n",
    "from keras.regularizers import L2\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def iterative_refinement(\n",
    "    aquifer_name: str = \"aquifer\",\n",
    "    imputed_data_pickle: str = \"source.pkl\",\n",
    "    output_file: str = \"output.pkl\",\n",
    "    timeseries_name: str = \"timeseries\",\n",
    "    locations_name: str = \"Locations\",\n",
    "    imputed_name: str = \"Data\",\n",
    "    weight_correlation: float = 0.70,\n",
    "    num_features: int = 5,\n",
    "    feature_threshold: float = 0.60,\n",
    "    n_iterations: int = 2,\n",
    "    validation_split: float = 0.3,\n",
    "    folds: int = 5,\n",
    "    regression_intercept_percentage: float = 0.10,\n",
    "    regression_percentages: List[float] = [0.10, 0.25, 0.5, 1.0],\n",
    "    windows: List[int] = [24],\n",
    "):\n",
    "    sys.path.append(\"..\")\n",
    "\n",
    "    # set weight of the distance correlation\n",
    "    weight_distance = 1 - weight_correlation\n",
    "\n",
    "    # set project arguments\n",
    "    project_args = utils_ml.ProjectSettings(\n",
    "        aquifer_name=aquifer_name,\n",
    "        iteration_current=1,\n",
    "        iteration_target=n_iterations,\n",
    "        artifacts_dir=None,\n",
    "    )\n",
    "\n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        filename=os.path.join(project_args.this_dir, \"error.log\"), level=logging.ERROR\n",
    "    )\n",
    "\n",
    "    for iteration in range(n_iterations):\n",
    "        logging.info(\n",
    "            f\"Starting iteration {project_args.iteration_current} of {n_iterations}\"\n",
    "        )\n",
    "\n",
    "        # create metrics class\n",
    "        metrics_class = utils_ml.Metrics(\n",
    "            validation_split=validation_split,\n",
    "            folds=folds,\n",
    "        )\n",
    "\n",
    "        # Load preprocessed data\n",
    "        data_dict = utils.load_pickle(\n",
    "            file_name=imputed_data_pickle,\n",
    "            directory=project_args.dataset_outputs_dir,\n",
    "        )\n",
    "        data_dict_well = data_dict[timeseries_name]\n",
    "        data_dict_locations = data_dict[locations_name]\n",
    "        data_dict_features = data_dict[imputed_name]\n",
    "\n",
    "        # create dictionary for model outputs\n",
    "        data_dict_well[\"runs\"] = {}\n",
    "\n",
    "        # create list of well ids and imputation range for dataframe creation\n",
    "        imputation_range = utils.make_interpolation_index()\n",
    "        well_ids = list(map(str, data_dict_well.columns))\n",
    "\n",
    "        # create summary metrics dataframe and imputation dataframe\n",
    "        imputation_df = pd.DataFrame(\n",
    "            index=imputation_range,\n",
    "        )\n",
    "        location_df = pd.DataFrame(\n",
    "            columns=[\"Longitude\", \"Latitude\"],\n",
    "        )\n",
    "        metrics_df = pd.DataFrame(\n",
    "            index=well_ids,\n",
    "            columns=metrics_class.metrics,\n",
    "        )\n",
    "        prediction_df = pd.DataFrame(\n",
    "            index=imputation_range,\n",
    "        )\n",
    "        correlation_df = pd.DataFrame(\n",
    "            index=well_ids,\n",
    "            columns=[\"feature_importance\", \"correlation_importance\", \"combined_score\"],\n",
    "        )\n",
    "\n",
    "        # start imputation loop\n",
    "        for i, well_id in tqdm(\n",
    "            enumerate(well_ids[0:2]),\n",
    "            total=len(well_ids[0:2]),\n",
    "            position=0,\n",
    "            leave=False,\n",
    "        ):\n",
    "            logging.info(f\"Starting imputation for well: {well_id}\")\n",
    "\n",
    "            try:\n",
    "                # setup well object to establish indecies\n",
    "                well_class = utils_ml.WellIndecies(\n",
    "                    well_id=well_id,\n",
    "                    timeseries=data_dict_well[well_id],\n",
    "                    location=pd.DataFrame(\n",
    "                        data_dict_locations.loc[well_id]\n",
    "                    ).T,\n",
    "                    imputation_range=imputation_range,\n",
    "                )\n",
    "\n",
    "                # parse raw and data series with corresponding indecies\n",
    "                y_raw = well_class.raw_series\n",
    "                y_data = well_class.data\n",
    "\n",
    "                # create one-hot encodings for observation month\n",
    "                table_dumbies = utils_ml.get_dummy_variables(y_data.index)\n",
    "\n",
    "                # create prior features, uses less windows for less leakage\n",
    "                prior, prior_features = utils_ml.generate_priors(\n",
    "                    y_data=y_data,\n",
    "                    indecies=well_class,\n",
    "                    regression_percentages=regression_percentages,\n",
    "                    regression_intercept_percentage=regression_intercept_percentage,\n",
    "                    windows=windows,\n",
    "                )\n",
    "\n",
    "                # drop existing well from pretrained data\n",
    "                feature_data = data_dict_features.drop(columns=[well_id], axis=1)\n",
    "                location_data = data_dict_locations.drop(well_id, axis=0)\n",
    "\n",
    "                # calculate feature correlations to target\n",
    "                feature_importance = utils_ml.calculate_feature_correlations(\n",
    "                    source_series=y_data,\n",
    "                    target_dataframe=feature_data,\n",
    "                )\n",
    "\n",
    "                distance_importance = utils_ml.calculate_distance_correlations(\n",
    "                    source_series=well_class.location,\n",
    "                    target_dataframe=location_data,\n",
    "                )\n",
    "\n",
    "                # calculate combined correlation\n",
    "                correlation_importance = pd.concat(\n",
    "                    [feature_importance, distance_importance], axis=1, \n",
    "                )\n",
    "\n",
    "                correlation_importance[\"combined_score\"] = (\n",
    "                    feature_importance * weight_correlation\n",
    "                    + distance_importance * weight_distance\n",
    "                )\n",
    "                \n",
    "                correlation_importance.sort_values(\n",
    "                    by=[\"combined_score\"], axis=0, ascending=False, inplace=True\n",
    "                )\n",
    "\n",
    "                # add correlation importance to correlation dataframe\n",
    "                correlation_df.loc[well_id] = correlation_importance.iloc[\n",
    "                    :, num_features\n",
    "                ].mean()\n",
    "\n",
    "                # select top features and determine if additional features are needed\n",
    "                sample_score = correlation_importance.loc[\n",
    "                    correlation_importance[0:num_features]\n",
    "                ][\"combined_score\"].mean()\n",
    "                if feature_threshold > sample_score:\n",
    "                    features = num_features + int(\n",
    "                        (feature_threshold - sample_score) * 10\n",
    "                    )\n",
    "                top_features = correlation_importance.index[0:features].to_list()\n",
    "\n",
    "                # collect feature dataframes into list: pdsi, gldas, prior features, sw, gwf, and dummies\n",
    "                tables_merge = [\n",
    "                    prior_features,\n",
    "                    table_dumbies,\n",
    "                    feature_data[top_features],\n",
    "                ]\n",
    "\n",
    "                # iteratively merge predictors dataframes and drop rows with missing values\n",
    "                merged_df = utils_ml.reduce(\n",
    "                    lambda left, right: pd.merge(\n",
    "                        left=left,\n",
    "                        right=right,\n",
    "                        left_index=True,\n",
    "                        right_index=True,\n",
    "                        how=\"left\",\n",
    "                    ),\n",
    "                    tables_merge,\n",
    "                ).dropna()\n",
    "\n",
    "                # merge features with labels\n",
    "                well_set = y_data.to_frame(name=well_id).join(merged_df, how=\"outer\")\n",
    "\n",
    "                # match timeseries index by droping missing rows not in the label set\n",
    "                well_set_clean = well_set.dropna()\n",
    "\n",
    "                # split dataframe into features and labels\n",
    "                y, x = utils_ml.dataframe_split(well_set_clean, well_id)\n",
    "\n",
    "                # specify features that will be passed through without scaling\n",
    "                features_to_pass = table_dumbies.columns.to_list()\n",
    "\n",
    "                # create scaler object\n",
    "                scaler_features = utils_ml.StandardScaler()\n",
    "                scaler_labels = utils_ml.StandardScaler()\n",
    "\n",
    "                # Create folds to split data into train, validation, and test sets\n",
    "                n_epochs = []\n",
    "                n_folds = folds\n",
    "                current_fold = 1\n",
    "\n",
    "                (y_kfold, x_kfold) = (y.to_numpy(), x.to_numpy())\n",
    "                kfold = utils_ml.KFold(n_splits=n_folds, shuffle=False)\n",
    "                temp_metrics = pd.DataFrame(columns=metrics_class.metrics)\n",
    "                model_runs = pd.DataFrame(index=imputation_range)\n",
    "\n",
    "                # train k-folds grab error metrics average results\n",
    "                logging.info(f\"Starting k-fold cross validation for well: {well_id}\")\n",
    "                for train_index, test_index in kfold.split(y_kfold, x_kfold):\n",
    "                    x_train, x_test = x.iloc[train_index, :], x.iloc[test_index, :]\n",
    "                    y_train, y_test = y.iloc[train_index], y.iloc[test_index, :]\n",
    "\n",
    "                    # Create validation and training sets\n",
    "                    x_train, x_val, y_train, y_val = utils_ml.train_test_split(\n",
    "                        x_train, y_train, test_size=0.30, random_state=42\n",
    "                    )\n",
    "\n",
    "                    # Create scaler for features and labels\n",
    "                    x_train, scaler_features = utils_ml.scaler_pipeline(\n",
    "                        x=x_train,\n",
    "                        scaler_object=scaler_features,\n",
    "                        features_to_pass=features_to_pass,\n",
    "                        train=True,\n",
    "                    )\n",
    "\n",
    "                    x_val = utils_ml.scaler_pipeline(\n",
    "                        x=x_val,\n",
    "                        scaler_object=scaler_features,\n",
    "                        features_to_pass=features_to_pass,\n",
    "                        train=False,\n",
    "                    )\n",
    "                    x_test = utils_ml.scaler_pipeline(\n",
    "                        x=x_test,\n",
    "                        scaler_object=scaler_features,\n",
    "                        features_to_pass=features_to_pass,\n",
    "                        train=False,\n",
    "                    )\n",
    "\n",
    "                    # create predictors for full time series\n",
    "                    x_pred_temp = utils_ml.scaler_pipeline(\n",
    "                        x=merged_df,\n",
    "                        scaler_object=scaler_features,\n",
    "                        features_to_pass=features_to_pass,\n",
    "                        train=False,\n",
    "                    )\n",
    "\n",
    "                    # Transform Y values\n",
    "                    y_train = pd.DataFrame(\n",
    "                        scaler_labels.fit_transform(y_train),\n",
    "                        index=y_train.index,\n",
    "                        columns=y_train.columns,\n",
    "                    )\n",
    "                    y_val = pd.DataFrame(\n",
    "                        scaler_labels.transform(y_val),\n",
    "                        index=y_val.index,\n",
    "                        columns=y_val.columns,\n",
    "                    )\n",
    "                    y_test = pd.DataFrame(\n",
    "                        scaler_labels.transform(y_test),\n",
    "                        index=y_test.index,\n",
    "                        columns=y_test.columns,\n",
    "                    )\n",
    "\n",
    "                    # Model Initialization\n",
    "                    hidden_nodes = 50\n",
    "                    opt = Adam(learning_rate=0.001)\n",
    "                    model = Sequential()\n",
    "                    model.add(\n",
    "                        Dense(\n",
    "                            hidden_nodes,\n",
    "                            input_dim=x_train.shape[1],\n",
    "                            activation=\"relu\",\n",
    "                            use_bias=True,\n",
    "                            kernel_initializer=\"glorot_uniform\",\n",
    "                            kernel_regularizer=L2(l2=0.1),\n",
    "                        )\n",
    "                    )\n",
    "                    model.add(Dropout(rate=0.2))\n",
    "                    model.add(\n",
    "                        Dense(\n",
    "                            2 * hidden_nodes,\n",
    "                            input_dim=x_train.shape[1],\n",
    "                            activation=\"relu\",\n",
    "                            use_bias=True,\n",
    "                            kernel_initializer=\"glorot_uniform\",\n",
    "                        )\n",
    "                    )\n",
    "                    model.add(Dropout(rate=0.2))\n",
    "                    model.add(Dense(1))\n",
    "                    model.compile(\n",
    "                        optimizer=opt,\n",
    "                        loss=\"mse\",\n",
    "                        metrics=[RootMeanSquaredError()],\n",
    "                    )\n",
    "\n",
    "                    # Hyper Paramter Adjustments\n",
    "                    early_stopping = callbacks.EarlyStopping(\n",
    "                        monitor=\"val_loss\",\n",
    "                        patience=5,\n",
    "                        min_delta=0.0,\n",
    "                        restore_best_weights=True,\n",
    "                    )\n",
    "\n",
    "                    adaptive_lr = callbacks.ReduceLROnPlateau(\n",
    "                        monitor=\"val_loss\", factor=0.1, min_lr=0\n",
    "                    )\n",
    "\n",
    "                    history = model.fit(\n",
    "                        x_train,\n",
    "                        y_train,\n",
    "                        epochs=700,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        verbose=0,\n",
    "                        callbacks=[early_stopping, adaptive_lr],\n",
    "                    )\n",
    "\n",
    "                    # Score and Tracking Metrics\n",
    "                    y_train = pd.DataFrame(\n",
    "                        scaler_labels.inverse_transform(y_train),\n",
    "                        index=y_train.index,\n",
    "                        columns=[\"Y Train\"],\n",
    "                    ).sort_index(axis=0, ascending=True)\n",
    "\n",
    "                    y_train_hat = pd.DataFrame(\n",
    "                        scaler_labels.inverse_transform(model.predict(x_train)),\n",
    "                        index=x_train.index,\n",
    "                        columns=[\"Y Train Hat\"],\n",
    "                    ).sort_index(axis=0, ascending=True)\n",
    "\n",
    "                    y_val = pd.DataFrame(\n",
    "                        scaler_labels.inverse_transform(y_val),\n",
    "                        index=y_val.index,\n",
    "                        columns=[\"Y Val\"],\n",
    "                    ).sort_index(axis=0, ascending=True)\n",
    "\n",
    "                    y_val_hat = pd.DataFrame(\n",
    "                        scaler_labels.inverse_transform(model.predict(x_val)),\n",
    "                        index=x_val.index,\n",
    "                        columns=[\"Y Val Hat\"],\n",
    "                    ).sort_index(axis=0, ascending=True)\n",
    "\n",
    "                    train_points, val_points = [len(y_train)], [len(y_val)]\n",
    "                    train_me = (\n",
    "                        sum(y_train_hat.values - y_train.values) / train_points\n",
    "                    ).item()\n",
    "                    train_rmse = mean_squared_error(\n",
    "                        y_train.values, y_train_hat.values, squared=False\n",
    "                    )\n",
    "                    train_mae = mean_absolute_error(y_train.values, y_train_hat.values)\n",
    "\n",
    "                    val_me = (sum(y_val_hat.values - y_val.values) / val_points).item()\n",
    "                    val_rmse = mean_squared_error(\n",
    "                        y_val.values, y_val_hat.values, squared=False\n",
    "                    )\n",
    "                    val_mae = mean_absolute_error(y_val.values, y_val_hat.values)\n",
    "\n",
    "                    train_e = [train_me, train_rmse, train_mae]\n",
    "                    val_e = [val_me, val_rmse, val_mae]\n",
    "\n",
    "                    test_cols = [\"Test ME\", \"Test RMSE\", \"Test MAE\"]\n",
    "\n",
    "                    train_errors = np.array([train_e + val_e]).reshape((1, 6))\n",
    "                    errors_col = [\n",
    "                        \"Train ME\",\n",
    "                        \"Train RMSE\",\n",
    "                        \"Train MAE\",\n",
    "                        \"Validation ME\",\n",
    "                        \"Validation RMSE\",\n",
    "                        \"Validation MAE\",\n",
    "                    ]\n",
    "                    df_metrics = pd.DataFrame(\n",
    "                        train_errors, index=([str(current_fold)]), columns=errors_col\n",
    "                    )\n",
    "\n",
    "                    df_metrics[\"Train Points\"] = train_points\n",
    "                    df_metrics[\"Validation Points\"] = val_points\n",
    "                    df_metrics[\"Train r2\"], _ = pearsonr(\n",
    "                        y_train.values.flatten(),\n",
    "                        y_train_hat.values.flatten(),\n",
    "                    )\n",
    "                    df_metrics[\"Validation r2\"], _ = pearsonr(\n",
    "                        y_val.values.flatten(),\n",
    "                        y_val_hat.values.flatten(),\n",
    "                    )\n",
    "                    temp_metrics = pd.concat(\n",
    "                        objs=[\n",
    "                            temp_metrics,\n",
    "                            df_metrics,\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "                    # Model Prediction\n",
    "                    prediction_temp = pd.DataFrame(\n",
    "                        scaler_labels.inverse_transform(model.predict(x_pred_temp)),\n",
    "                        index=x_pred_temp.index,\n",
    "                        columns=[current_fold],\n",
    "                    )\n",
    "\n",
    "                    # append prediction to model runs\n",
    "                    model_runs = model_runs.join(prediction_temp, how=\"outer\")\n",
    "\n",
    "                    # Test Sets and Plots\n",
    "                    try:\n",
    "                        y_test = pd.DataFrame(\n",
    "                            scaler_labels.inverse_transform(y_test),\n",
    "                            index=y_test.index,\n",
    "                            columns=[\"Y Test\"],\n",
    "                        ).sort_index(axis=0, ascending=True)\n",
    "                        y_test_hat = pd.DataFrame(\n",
    "                            scaler_labels.inverse_transform(model.predict(x_test)),\n",
    "                            index=y_test.index,\n",
    "                            columns=[\"Y Test Hat\"],\n",
    "                        ).sort_index(axis=0, ascending=True)\n",
    "                        test_points = len(y_test)\n",
    "                        test_me = (\n",
    "                            sum(y_test_hat.values - y_test.values) / test_points\n",
    "                        ).item()\n",
    "                        test_rmse = mean_squared_error(\n",
    "                            y_test.values, y_test_hat.values, squared=False\n",
    "                        )\n",
    "                        test_mae = mean_absolute_error(y_test.values, y_test_hat.values)\n",
    "\n",
    "                        test_errors = np.array([test_me, test_rmse, test_mae]).reshape(\n",
    "                            (1, 3)\n",
    "                        )\n",
    "                        test_cols = [\"Test ME\", \"Test RMSE\", \"Test MAE\"]\n",
    "                        test_metrics = pd.DataFrame(\n",
    "                            test_errors, index=[str(current_fold)], columns=test_cols\n",
    "                        )\n",
    "                        test_metrics[\"Test Points\"] = test_points\n",
    "                        test_metrics[\"Test r2\"], _ = pearsonr(\n",
    "                            y_test.values.flatten(), y_test_hat.values.flatten()\n",
    "                        )\n",
    "                        temp_metrics.loc[\n",
    "                            str(current_fold), test_metrics.columns\n",
    "                        ] = test_metrics.loc[str(current_fold)]\n",
    "\n",
    "                    except:\n",
    "                        temp_metrics.loc[\n",
    "                            str(current_fold), [\"Test ME\", \"Test RMSE\", \"Test MAE\"]\n",
    "                        ] = np.NAN\n",
    "                        temp_metrics.loc[str(current_fold), \"Test Points\"] = 0\n",
    "                        temp_metrics.loc[str(current_fold), \"Test r2\"] = np.NAN\n",
    "\n",
    "                    current_fold += 1\n",
    "                    n_epochs.append(len(history.history[\"loss\"]))\n",
    "\n",
    "                # Log metrics\n",
    "                logging.info(f\"Finished k-fold cross validation for well: {well_id}\")\n",
    "                logging.info(f\"Starting model training for well: {well_id}\")\n",
    "                epochs = int(sum(n_epochs) / n_folds)\n",
    "\n",
    "                # Reset feature scalers\n",
    "                x, scaler_features = utils_ml.scaler_pipeline(\n",
    "                    x,\n",
    "                    scaler_features,\n",
    "                    features_to_pass,\n",
    "                    train=True,\n",
    "                )\n",
    "                x_pred = utils_ml.scaler_pipeline(\n",
    "                    merged_df,\n",
    "                    scaler_features,\n",
    "                    features_to_pass,\n",
    "                    train=False,\n",
    "                )\n",
    "                y = pd.DataFrame(\n",
    "                    scaler_labels.fit_transform(y),\n",
    "                    index=y.index,\n",
    "                    columns=y.columns,\n",
    "                )\n",
    "\n",
    "                # Retrain Model with number of epochs\n",
    "                history = model.fit(x, y, epochs=epochs, verbose=0)\n",
    "                metrics_avg = pd.DataFrame(\n",
    "                    temp_metrics.mean(), columns=[well_id]\n",
    "                ).transpose()\n",
    "                metrics_df = pd.concat(objs=[metrics_df, metrics_avg])\n",
    "\n",
    "                # Model Prediction\n",
    "                prediction = pd.DataFrame(\n",
    "                    scaler_labels.inverse_transform(model.predict(x_pred)).astype(\n",
    "                        float\n",
    "                    ),\n",
    "                    index=x_pred.index,\n",
    "                    columns=[well_id],\n",
    "                )\n",
    "\n",
    "                # append location to location dataframe\n",
    "                location_df = pd.concat(\n",
    "                    [location_df, well_class.location], axis=0, ignore_index=False\n",
    "                )\n",
    "\n",
    "                # append prediction to prediction dataframe\n",
    "                prediction_df = prediction_df.join(prediction, how=\"outer\")\n",
    "\n",
    "                # append prediction to model runs\n",
    "                model_runs = model_runs.join(prediction, how=\"outer\")\n",
    "                data_dict_well[\"runs\"][well_id] = model_runs\n",
    "\n",
    "                # calculate spread and comp r2\n",
    "                spread = pd.DataFrame(index=prediction.index, columns=[\"mean\", \"std\"])\n",
    "                spread[\"mean\"] = model_runs.mean(axis=1)\n",
    "                spread[\"std\"] = model_runs.std(axis=1)\n",
    "                comp_r2 = r2_score(\n",
    "                    scaler_labels.inverse_transform(y.values.reshape(-1, 1)),\n",
    "                    scaler_labels.inverse_transform(model.predict(x)),\n",
    "                )\n",
    "                metrics_df.loc[well_id, \"Comp R2\"] = comp_r2\n",
    "\n",
    "                # Data Filling\n",
    "                gap_time_series = pd.DataFrame(\n",
    "                    data_dict_well[\"timeseries\"][well_id], index=prediction.index\n",
    "                )\n",
    "                filled_time_series = gap_time_series[well_id].fillna(\n",
    "                    prediction[well_id]\n",
    "                )\n",
    "                if y_raw.dropna().index[-1] > prediction.index[-1]:\n",
    "                    filled_time_series = pd.concat(\n",
    "                        [filled_time_series, y_raw.dropna()], join=\"outer\", axis=1\n",
    "                    )\n",
    "                    filled_time_series = filled_time_series.iloc[:, 0]\n",
    "                    filled_time_series = filled_time_series.fillna(y_raw)\n",
    "                imputation_df = imputation_df.join(filled_time_series, how=\"outer\")\n",
    "                logging.info(f\"Finished model training for well: {well_id}\")\n",
    "\n",
    "            except (\n",
    "                ValueError,\n",
    "                TypeError,\n",
    "                KeyError,\n",
    "                IndexError,\n",
    "                FileNotFoundError,\n",
    "                PermissionError,\n",
    "                ConnectionError,\n",
    "                Exception,\n",
    "            ) as error:\n",
    "                logging.error(f\"An exception ocurred on well: {well_id}\\n{error}\")\n",
    "                logging.error(traceback.format_exc())\n",
    "                continue\n",
    "\n",
    "        # build output dictionary\n",
    "        data_dict_well[\"Data\"] = imputation_df.loc[imputation_range]\n",
    "        data_dict_well[\"Predictions\"] = prediction_df.loc[imputation_range]\n",
    "        data_dict_well[\"Locations\"] = location_df\n",
    "        data_dict_well[\"Metrics\"] = metrics_df\n",
    "        data_dict_well[\"Correlations\"] = correlation_df\n",
    "\n",
    "        utils.save_pickle(\n",
    "            data_dict_well,\n",
    "            f\"{output_file}_{iteration}.pickle\",\n",
    "            project_args.dataset_outputs_dir,\n",
    "        )\n",
    "        logging.info(f\"Finished imputation for {aquifer_name} aquifer\")\n",
    "        logging.info(\n",
    "            \"Added the following data to the data dictionary: Data, Predictions, Locations, Metrics, Correlations\"\n",
    "        )\n",
    "        logging.info(f\"Saved data dictionary to {output_file}_{iteration}\")\n",
    "\n",
    "        # update project arguments\n",
    "        project_args.iteration_current = iteration + 1\n",
    "\n",
    "data_path = \"/home/saul/workspace/groundwater_well_imputation/groundwater_imputation_api/src/imputation_api/artifacts/dataset_outputs/beryl_enterpris_imputation_iteration_1.pickle\"\n",
    "\n",
    "iterative_refinement(\n",
    "    aquifer_name = \"Beryl Enterprise\",\n",
    "    imputed_data_pickle = \"beryl_enterpris_imputation_iteration_0.pickle\",\n",
    "    output_file = \"beryl_enterprise_well_data_imputation_iteration.pickle\",\n",
    "    validation_split = 0.3,\n",
    "    folds = 5,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groundwater_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
