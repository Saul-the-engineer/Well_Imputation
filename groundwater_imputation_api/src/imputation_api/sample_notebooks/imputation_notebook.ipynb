{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import utils_preprocess\n",
    "import utils_spatial_interpolation\n",
    "import utils_spatial_analysis\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile\n",
    "path_shape = '/home/saul/workspace/Well_Imputation/groundwater_imputation_api/src/imputation_api/artifacts/aquifer_shapes/Beryl_Enterprise.shp'\n",
    "aquifer_shape = utils.load_shapefile(path=path_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Preprocessing PDSI and GLDAS data into Tabular Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the pdsi netcdf files to obtain tabular data pickle file\n",
    "pdsi_source_directory = r'C:\\Users\\saulg\\Desktop\\Remote_Data\\pdsi'\n",
    "pdsi_target_directory = r'C:\\Users\\saulg\\Desktop\\Remote_Data\\pdsi_tabular'\n",
    "\n",
    "utils_preprocess.process_pdsi_data(\n",
    "    source_directory=pdsi_source_directory, \n",
    "    target_directory=pdsi_target_directory,\n",
    "    date_start='01/01/1850',\n",
    "    date_end='12/31/2020',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the gldas netcdf files to obtain tabular data pickle file\n",
    "gldas_source_directory = r'C:\\Users\\saulg\\Desktop\\Remote_Data\\GLDAS'\n",
    "gldas_target_directory = r'C:\\Users\\saulg\\Desktop\\Remote_Data\\gldas_tabular'\n",
    "\n",
    "utils_preprocess.process_gldas_data(\n",
    "    source_directory=gldas_source_directory, \n",
    "    target_directory=gldas_target_directory,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Transform PDSI, GLDAS, and Well Observations into format for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse pdsi data and save it\n",
    "directory_pdsi = r\"/mnt/c/Users/saulg/Desktop/Remote_Data/pdsi_tabular\"\n",
    "pdsi:dict = utils.pull_relevant_data(\n",
    "    shape=aquifer_shape, \n",
    "    dataset_name=\"PDSI\", \n",
    "    dataset_directory=directory_pdsi\n",
    "    )\n",
    "utils.save_pickle(\n",
    "    data=pdsi, \n",
    "    file_name=\"pdsi_data.pickle\", \n",
    "    directory=\"/home/saul/workspace/Well_Imputation/groundwater_imputation_api/src/imputation_api/artifacts/dataset_outputs\",\n",
    "    protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the GLDAS data and save it\n",
    "directory_gldas = r\"/mnt/c/Users/saulg/Desktop/Remote_Data/gldas_tabular\"\n",
    "gldas:dict = utils.pull_relevant_data(\n",
    "    shape=aquifer_shape, \n",
    "    dataset_name=\"GLDAS\", \n",
    "    dataset_directory=directory_gldas\n",
    "    )\n",
    "utils.save_pickle(\n",
    "    data=gldas, \n",
    "    file_name=\"gldas_data.pickle\", \n",
    "    directory=\"/home/saul/workspace/Well_Imputation/groundwater_imputation_api/src/imputation_api/artifacts/dataset_outputs\",\n",
    "    protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process well data from csv files\n",
    "well_locations = pd.read_csv(\"/home/saul/workspace/Well_Imputation/groundwater_imputation_api/src/imputation_api/artifacts/aquifer_data/EscalanteBerylLocation.csv\")\n",
    "well_timeseries = pd.read_csv(\"/home/saul/workspace/Well_Imputation/groundwater_imputation_api/src/imputation_api/artifacts/aquifer_data/EscalanteBerylTimeseries.csv\")\n",
    "data:dict = utils.transform_well_data(\n",
    "    well_timeseries=well_timeseries, \n",
    "    well_locations=well_locations,\n",
    "    timeseries_name=\"timeseries\",\n",
    "    locations_name=\"locations\",\n",
    "    )\n",
    "utils.save_pickle(\n",
    "    data=data, \n",
    "    file_name=\"beryl_enterprise_data.pickle\", \n",
    "    directory=\"/home/saul/workspace/Well_Imputation/groundwater_imputation_api/src/imputation_api/artifacts/dataset_outputs\", \n",
    "    protocol=3,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the timeseries data to see if it looks reasonable\n",
    "plt.plot(data[\"timeseries\"], '-.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Develop initial imputation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Develop iterative refinement model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Analyze spatial characteristics of imputation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/saul/workspace/Well_Imputation/groundwater_imputation_api/src/imputation_api/artifacts/dataset_outputs/beryl_enterprise_well_data_imputation_iteration_1.pickle\"\n",
    "\n",
    "utils_spatial_interpolation.kriging_interpolation(\n",
    "    data_pickle_path = data_path,\n",
    "    shape_file_path = path_shape,\n",
    "    n_x_cells=100,\n",
    "    influence_distance=0.125,\n",
    "    monthly_time_step=1,\n",
    "    netcdf_filename=\"beryl_enterprise_spatial_analysis_iteration_1.nc\",\n",
    "    directory=\"/home/saul/workspace/Well_Imputation/groundwater_imputation_api/src/imputation_api/artifacts/dataset_outputs\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Calculate Storage Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster = nc.Dataset(\"/home/saul/workspace/Well_Imputation/groundwater_imputation_api/src/imputation_api/artifacts/dataset_outputs/beryl_enterprise_spatial_analysis_iteration_1.nc\", 'r')\n",
    "\n",
    "spatial_analysis = utils_spatial_analysis.StorageChangeCalculator(\n",
    "    units=\"English\",\n",
    "    storage_coefficient=0.2,\n",
    "    anisotropic=\"x\",\n",
    ")\n",
    "storage_change = spatial_analysis.calulate_storage_curve(\n",
    "    raster=raster, \n",
    "    date_range_filter=(\"1948-01-01\", \"1978-01-01\"), # if you need to filter dates outside of original time range\n",
    "    )\n",
    "\n",
    "plt.plot(storage_change, '-.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Geospatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
