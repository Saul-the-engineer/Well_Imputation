{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 02:35:22.914576: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-17 02:35:22.949601: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-17 02:35:22.950254: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-17 02:35:23.753122: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import utils_preprocess\n",
    "import utils_spatial_interpolation\n",
    "import utils_spatial_analysis\n",
    "\n",
    "from utils_satellite_imputation import satellite_imputation\n",
    "from utils_iterative_refinement import iterative_refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Load shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile\n",
    "path_shape = '/home/saul/workspace/groundwater_well_imputation/groundwater_imputation_api/src/imputation_api/artifacts/aquifer_shapes/Beryl_Enterprise.shp'\n",
    "aquifer_shape = utils.load_shapefile(path=path_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Preprocessing PDSI and GLDAS data into Tabular Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the pdsi netcdf files to obtain tabular data pickle file\n",
    "pdsi_source_directory = r'C:\\Users\\saulg\\Desktop\\Remote_Data\\pdsi'\n",
    "pdsi_target_directory = r'C:\\Users\\saulg\\Desktop\\Remote_Data\\pdsi_tabular'\n",
    "\n",
    "utils_preprocess.process_pdsi_data(\n",
    "    source_directory=pdsi_source_directory, \n",
    "    target_directory=pdsi_target_directory,\n",
    "    date_start='01/01/1850',\n",
    "    date_end='12/31/2020',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the gldas netcdf files to obtain tabular data pickle file\n",
    "gldas_source_directory = r'C:\\Users\\saulg\\Desktop\\Remote_Data\\GLDAS'\n",
    "gldas_target_directory = r'C:\\Users\\saulg\\Desktop\\Remote_Data\\gldas_tabular'\n",
    "\n",
    "utils_preprocess.process_gldas_data(\n",
    "    source_directory=gldas_source_directory, \n",
    "    target_directory=gldas_target_directory,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Transform PDSI, GLDAS, and Well Observations into format for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process PDSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse pdsi data and save it\n",
    "directory_pdsi = r\"/mnt/c/Users/saulg/Desktop/Remote_Data/pdsi_tabular\"\n",
    "\n",
    "pdsi:dict = utils.pull_relevant_data(\n",
    "    shape=aquifer_shape, \n",
    "    dataset_name=\"PDSI\", \n",
    "    dataset_directory=directory_pdsi\n",
    "    )\n",
    "\n",
    "utils.save_pickle(\n",
    "    data=pdsi, \n",
    "    file_name=\"pdsi_data.pickle\", \n",
    "    directory=\"/home/saul/workspace/Well_Imputation/groundwater_imputation_api/src/imputation_api/artifacts/dataset_outputs\",\n",
    "    protocol=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process GLDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the GLDAS data and save it\n",
    "directory_gldas = r\"/mnt/c/Users/saulg/Desktop/Remote_Data/gldas_tabular\"\n",
    "\n",
    "gldas:dict = utils.pull_relevant_data(\n",
    "    shape=aquifer_shape, \n",
    "    dataset_name=\"GLDAS\", \n",
    "    dataset_directory=directory_gldas\n",
    "    )\n",
    "\n",
    "utils.save_pickle(\n",
    "    data=gldas, \n",
    "    file_name=\"gldas_data.pickle\", \n",
    "    directory=\"/home/saul/workspace/Well_Imputation/groundwater_imputation_api/src/imputation_api/artifacts/dataset_outputs\",\n",
    "    protocol=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Well Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process well data from csv files\n",
    "well_locations = pd.read_csv(\"/home/saul/workspace/groundwater_well_imputation/groundwater_imputation_api/src/imputation_api/artifacts/aquifer_data/EscalanteBerylLocation.csv\")\n",
    "well_timeseries = pd.read_csv(\"/home/saul/workspace/groundwater_well_imputation/groundwater_imputation_api/src/imputation_api/artifacts/aquifer_data/EscalanteBerylTimeseries.csv\")\n",
    "\n",
    "data:dict = utils.transform_well_data(\n",
    "    well_timeseries=well_timeseries, \n",
    "    well_locations=well_locations,\n",
    "    timeseries_name=\"timeseries\",\n",
    "    locations_name=\"locations\",\n",
    "    )\n",
    "\n",
    "utils.save_pickle(\n",
    "    data=data, \n",
    "    file_name=\"beryl_enterprise_data.pickle\", \n",
    "    directory=\"/home/saul/workspace/Well_Imputation/groundwater_imputation_api/src/imputation_api/artifacts/dataset_outputs\", \n",
    "    protocol=3,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the timeseries data to see if it looks reasonable\n",
    "plt.plot(data[\"timeseries\"], '-.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Develop initial imputation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satellite_imputation(\n",
    "    aquifer_name = \"Beryl Enterprise\",\n",
    "    pdsi_pickle = \"/home/saul/workspace/groundwater_well_imputation/groundwater_imputation_api/src/imputation_api/artifacts/dataset_outputs/pdsi_data.pickle\",\n",
    "    gldas_pickle = \"/home/saul/workspace/groundwater_well_imputation/groundwater_imputation_api/src/imputation_api/artifacts/dataset_outputs/gldas_data.pickle\",\n",
    "    well_data_pickle = \"/home/saul/workspace/groundwater_well_imputation/groundwater_imputation_api/src/imputation_api/artifacts/dataset_outputs/beryl_enterprise_data.pickle\",\n",
    "    output_file = \"beryl_enterpris_imputation_satellite.pickle\",\n",
    "    timeseries_name=\"timeseries\",\n",
    "    locations_name=\"locations\",\n",
    "    validation_split = 0.3,\n",
    "    folds = 5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Develop iterative refinement model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting iteration 1 of 2\n",
      "INFO:utils:Pickle file 'beryl_enterpris_imputation_satellite.pickle' loaded successfully from '/home/saul/workspace/groundwater_well_imputation/groundwater_imputation_api/src/imputation_api/artifacts/dataset_outputs'\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:Starting imputation for well: 373338113431502\n",
      "INFO:root:Starting k-fold cross validation for well: 373338113431502\n",
      "2023-12-17 02:35:34.303299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-17 02:35:34.325741: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 899us/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 929us/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 982us/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 888us/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 869us/step\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Finished k-fold cross validation for well: 373338113431502\n",
      "INFO:root:Starting model training for well: 373338113431502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 981us/step\n",
      "16/16 [==============================] - 0s 927us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Finished model training for well: 373338113431502\n",
      " 50%|█████     | 1/2 [00:17<00:17, 17.64s/it]INFO:root:Starting imputation for well: 373418113430601\n",
      "INFO:root:Starting k-fold cross validation for well: 373418113430601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "28/28 [==============================] - 0s 991us/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "28/28 [==============================] - 0s 937us/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "28/28 [==============================] - 0s 853us/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "28/28 [==============================] - 0s 906us/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "28/28 [==============================] - 0s 890us/step\n",
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Finished k-fold cross validation for well: 373418113430601\n",
      "INFO:root:Starting model training for well: 373418113430601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 866us/step\n",
      "5/5 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Finished model training for well: 373418113430601\n",
      "INFO:utils:Pickle file 'beryl_enterprise_iterative_1.pickle' saved successfully to '/home/saul/workspace/groundwater_well_imputation/groundwater_imputation_api/src/imputation_api/artifacts/dataset_outputs'\n",
      "INFO:root:Finished imputation for Beryl Enterprise aquifer\n",
      "INFO:root:Added the following data to the data dictionary: Data, Predictions, Locations, Metrics, Correlations\n",
      "INFO:root:Saved data dictionary to beryl_enterprise_iterative_1.pickle\n",
      "INFO:root:Starting iteration 2 of 2\n",
      "INFO:utils:Pickle file 'beryl_enterpris_imputation_satellite.pickle' loaded successfully from '/home/saul/workspace/groundwater_well_imputation/groundwater_imputation_api/src/imputation_api/artifacts/dataset_outputs'\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:Starting imputation for well: 373338113431502\n",
      "INFO:root:Starting k-fold cross validation for well: 373338113431502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 954us/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 894us/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 893us/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 870us/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 986us/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 822us/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 853us/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Finished k-fold cross validation for well: 373338113431502\n",
      "INFO:root:Starting model training for well: 373338113431502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Finished model training for well: 373338113431502\n",
      " 50%|█████     | 1/2 [00:18<00:18, 18.48s/it]INFO:root:Starting imputation for well: 373418113430601\n",
      "INFO:root:Starting k-fold cross validation for well: 373418113430601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "28/28 [==============================] - 0s 872us/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "28/28 [==============================] - 0s 907us/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "28/28 [==============================] - 0s 950us/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "28/28 [==============================] - 0s 933us/step\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Finished k-fold cross validation for well: 373418113430601\n",
      "INFO:root:Starting model training for well: 373418113430601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Finished model training for well: 373418113430601\n",
      "INFO:utils:Pickle file 'beryl_enterprise_iterative_2.pickle' saved successfully to '/home/saul/workspace/groundwater_well_imputation/groundwater_imputation_api/src/imputation_api/artifacts/dataset_outputs'\n",
      "INFO:root:Finished imputation for Beryl Enterprise aquifer\n",
      "INFO:root:Added the following data to the data dictionary: Data, Predictions, Locations, Metrics, Correlations\n",
      "INFO:root:Saved data dictionary to beryl_enterprise_iterative_2.pickle\n"
     ]
    }
   ],
   "source": [
    "iterative_refinement(\n",
    "    aquifer_name = \"Beryl Enterprise\",\n",
    "    imputed_data_pickle = \"beryl_enterpris_imputation_satellite.pickle\",\n",
    "    output_file = \"beryl_enterprise_iterative.pickle\",\n",
    "    validation_split = 0.3,\n",
    "    folds = 5,\n",
    "    feature_threshold = 0.60,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Analyze spatial characteristics of imputation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_spatial_interpolation.kriging_interpolation(\n",
    "    data_pickle_path = \"/home/saul/workspace/groundwater_well_imputation/groundwater_imputation_api/src/imputation_api/artifacts/dataset_outputs/beryl_enterpris_imputation_iteration_1.pickle\",\n",
    "    shape_file_path = '/home/saul/workspace/groundwater_well_imputation/groundwater_imputation_api/src/imputation_api/artifacts/aquifer_shapes/Beryl_Enterprise.shp',\n",
    "    n_x_cells=100,\n",
    "    influence_distance=0.125,\n",
    "    monthly_time_step=1,\n",
    "    netcdf_filename=\"beryl_enterprise_spatial_analysis_iteration_1.nc\",\n",
    "    directory=\"/home/saul/workspace/groundwater_well_imputation/groundwater_imputation_api/src/imputation_api/artifacts/dataset_outputs\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Calculate Storage Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster = nc.Dataset(\n",
    "    \"/home/saul/workspace/groundwater_well_imputation/groundwater_imputation_api/src/imputation_api/artifacts/dataset_outputs/beryl_enterprise_spatial_analysis_iteration_1.nc\",\n",
    "    'r',\n",
    "    )\n",
    "\n",
    "spatial_analysis = utils_spatial_analysis.StorageChangeCalculator(\n",
    "    units=\"English\",\n",
    "    storage_coefficient=0.2,\n",
    "    anisotropic=\"x\",\n",
    ")\n",
    "storage_change = spatial_analysis.calulate_storage_curve(\n",
    "    raster=raster, \n",
    "    #date_range_filter=(\"1948-01-01\", \"1978-01-01\"), # if you need to filter dates within of original time range\n",
    "    )\n",
    "\n",
    "plt.plot(storage_change, '-.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Geospatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
