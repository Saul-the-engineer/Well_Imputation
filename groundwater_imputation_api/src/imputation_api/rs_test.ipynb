{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import utils\n",
    "import utils_ml\n",
    "import logging\n",
    "import traceback\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "from keras import callbacks\n",
    "from keras.regularizers import L2\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# set project arguments\n",
    "project_args = utils_ml.ProjectSettings(\n",
    "    aquifer_name=\"Beryl-Enterprise-Utah\",\n",
    "    iteration_current=1,\n",
    "    iteration_target=3,\n",
    "    artifacts_dir=None,\n",
    ")\n",
    "# create metrics class\n",
    "metrics_class = utils_ml.Metrics(\n",
    "    validation_split=0.3,\n",
    "    folds=5,\n",
    ")\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(project_args.this_dir, \"error.log\"), level=logging.ERROR\n",
    ")\n",
    "# Load preprocessed data\n",
    "data_dict_pdsi = utils.load_pickle(\n",
    "    file_name=\"pdsi_data.pickle\",\n",
    "    directory=project_args.dataset_outputs_dir,\n",
    ")\n",
    "data_dict_gldas = utils.load_pickle(\n",
    "    file_name=\"gldas_data.pickle\",\n",
    "    directory=project_args.dataset_outputs_dir,\n",
    ")\n",
    "data_dict_well = utils.load_pickle(\n",
    "    file_name=\"beryl_enterprise_data.pickle\",\n",
    "    directory=project_args.dataset_outputs_dir,\n",
    ")\n",
    "data_dict_well[\"runs\"] = {}\n",
    "# create list of well ids and imputation range for dataframe creation\n",
    "imputation_range = utils.make_interpolation_index()\n",
    "well_ids = list(map(str, data_dict_well[\"timeseries\"].columns))\n",
    "# create summary metrics dataframe and imputation dataframe\n",
    "summary_metrics_df = pd.DataFrame(\n",
    "    index=well_ids,\n",
    "    columns=metrics_class.metrics,\n",
    ")\n",
    "imputation_df = pd.DataFrame(\n",
    "    index=imputation_range,\n",
    ")\n",
    "model_output_df = pd.DataFrame(\n",
    "    index=imputation_range,\n",
    ")\n",
    "#well_ids = well_ids[2:3]\n",
    "\n",
    "# start imputation loop\n",
    "for i, well_id in tqdm(\n",
    "    enumerate(well_ids), total=len(well_ids), position=0, leave=False\n",
    "):\n",
    "    logging.info(f\"Starting imputation for well: {well_id}\")\n",
    "    try:\n",
    "        well_class = utils_ml.WellIndecies(\n",
    "            well_id=well_id,\n",
    "            timeseries=data_dict_well[\"timeseries\"][well_id],\n",
    "            location=pd.DataFrame(data_dict_well[\"locations\"].loc[well_id]).T,\n",
    "            imputation_range=imputation_range,\n",
    "        )\n",
    "        y_raw = well_class.raw_series\n",
    "        y_data = well_class.data\n",
    "        # create dummy variables for location\n",
    "        table_dumbies = utils_ml.get_dummy_variables(y_data.index)\n",
    "        # create prior features\n",
    "        prior, prior_features = utils_ml.generate_priors(\n",
    "            y_data=y_data,\n",
    "            indecies=well_class,\n",
    "            regression_percentages=[0.10, 0.15, 0.25, 0.5, 1.0],\n",
    "            regression_intercept_percentage=0.10,\n",
    "            windows=[18, 24, 36, 60],\n",
    "        )\n",
    "        # pdsi data cell selection\n",
    "        table_pdsi = utils_ml.remote_sensing_data_selection(\n",
    "            data_dict=data_dict_pdsi,\n",
    "            location_key=\"locations\",\n",
    "            location_query=well_class.location,\n",
    "        )\n",
    "        # gldas data cell selection\n",
    "        table_gldas = utils_ml.remote_sensing_data_selection(\n",
    "            data_dict=data_dict_gldas,\n",
    "            location_key=\"locations\",\n",
    "            location_query=well_class.location,\n",
    "        )\n",
    "        # subset GLDAS data\n",
    "        table_gldas = table_gldas[\n",
    "            [\n",
    "                \"Psurf_f_inst\",\n",
    "                \"Wind_f_inst\",\n",
    "                \"Qair_f_inst\",\n",
    "                \"Qh_tavg\",\n",
    "                \"Qsb_acc\",\n",
    "                \"PotEvap_tavg\",\n",
    "                \"Tair_f_inst\",\n",
    "                \"Rainf_tavg\",\n",
    "                \"SoilMoi0_10cm_inst\",\n",
    "                \"SoilMoi10_40cm_inst\",\n",
    "                \"SoilMoi40_100cm_inst\",\n",
    "                \"SoilMoi100_200cm_inst\",\n",
    "                \"CanopInt_inst\",\n",
    "                \"SWE_inst\",\n",
    "                \"Lwnet_tavg\",\n",
    "                \"Swnet_tavg\",\n",
    "            ]\n",
    "        ]\n",
    "        # calculate surface water feature from GLDAS\n",
    "        sw_names = [\n",
    "            \"SoilMoi0_10cm_inst\",\n",
    "            \"SoilMoi10_40cm_inst\",\n",
    "            \"SoilMoi40_100cm_inst\",\n",
    "            \"SoilMoi100_200cm_inst\",\n",
    "            \"CanopInt_inst\",\n",
    "            \"SWE_inst\",\n",
    "        ]\n",
    "        table_sw = pd.DataFrame(\n",
    "            table_gldas[sw_names].sum(axis=1).rename(\"Surface Water\")\n",
    "        )\n",
    "        # generate additional groundwater features\n",
    "        gw_names = [\"Qsb_acc\", \"SWE_inst\", \"Rainf_tavg\"]\n",
    "        table_gwf = (\n",
    "            table_gldas[gw_names]\n",
    "            .assign(\n",
    "                **{\n",
    "                    \"ln(QSB_acc)\": np.log(table_gldas[\"Qsb_acc\"]),\n",
    "                    \"ln(RW 4 Rainf_tavg)\": np.log(\n",
    "                        table_gldas[\"Rainf_tavg\"].rolling(4, min_periods=1).sum()\n",
    "                    ),\n",
    "                    \"Sum Soil Moist\": (\n",
    "                        table_sw.squeeze()\n",
    "                        - table_gldas[\"CanopInt_inst\"]\n",
    "                        - table_gldas[\"SWE_inst\"]\n",
    "                    )\n",
    "                    .rolling(3, min_periods=1)\n",
    "                    .sum(),\n",
    "                }\n",
    "            )\n",
    "            .drop(columns=gw_names)\n",
    "        )\n",
    "        # collect feature dataframes into list: pdsi, gldas, prior features, sw, gwf, and dummies\n",
    "        tables_merge = [\n",
    "            table_pdsi,\n",
    "            table_gldas,\n",
    "            prior_features,\n",
    "            table_sw,\n",
    "            table_gwf,\n",
    "            table_dumbies,\n",
    "        ]\n",
    "\n",
    "        # iteratively merge predictors dataframes and drop rows with missing values\n",
    "        merged_df = utils_ml.reduce(\n",
    "            lambda left, right: pd.merge(\n",
    "                left=left,\n",
    "                right=right,\n",
    "                left_index=True,\n",
    "                right_index=True,\n",
    "                how=\"left\",\n",
    "            ),\n",
    "            tables_merge,\n",
    "        ).dropna()\n",
    "\n",
    "        # merge features with labels\n",
    "        well_set = y_data.to_frame(name=well_id).join(merged_df, how=\"outer\")\n",
    "\n",
    "        # match timeseries index by droping missing rows not in the label set\n",
    "        well_set_clean = well_set.dropna()\n",
    "\n",
    "        # split dataframe into features and labels\n",
    "        y, x = utils_ml.dataframe_split(well_set_clean, well_id)\n",
    "\n",
    "        # specify features that will be scaled with StandardScaler\n",
    "        features_to_scale = (\n",
    "            prior_features.columns.to_list()\n",
    "            + table_sw.columns.to_list()\n",
    "            + table_gwf.columns.to_list()\n",
    "        )\n",
    "\n",
    "        # specify features that will be passed through without scaling\n",
    "        features_to_pass = table_dumbies.columns.to_list()\n",
    "\n",
    "        # create scaler object\n",
    "        scaler_features = utils_ml.StandardScaler()\n",
    "        scaler_labels = utils_ml.StandardScaler()\n",
    "\n",
    "        # Create folds to split data into train, validation, and test sets\n",
    "        n_epochs = []\n",
    "        n_folds = 5\n",
    "        current_fold = 1\n",
    "\n",
    "        (y_kfold, x_kfold) = (y.to_numpy(), x.to_numpy())\n",
    "        kfold = utils_ml.KFold(n_splits=n_folds, shuffle=False)\n",
    "        temp_metrics = pd.DataFrame(columns=metrics_class.metrics)\n",
    "        model_runs = pd.DataFrame(index=imputation_range)\n",
    "\n",
    "        # train k-folds grab error metrics average results\n",
    "        logging.info(f\"Starting k-fold cross validation for well: {well_id}\")\n",
    "        for train_index, test_index in kfold.split(y_kfold, x_kfold):\n",
    "            x_train, x_test = x.iloc[train_index, :], x.iloc[test_index, :]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index, :]\n",
    "\n",
    "            # Create validation and training sets\n",
    "            x_train, x_val, y_train, y_val = utils_ml.train_test_split(\n",
    "                x_train, y_train, test_size=0.30, random_state=42\n",
    "            )\n",
    "\n",
    "            # Create scaler for features and labels\n",
    "            x_train, scaler_features = utils_ml.scaler_pipeline(\n",
    "                x_train, scaler_features, features_to_pass, train=True\n",
    "            )\n",
    "            x_val = utils_ml.scaler_pipeline(\n",
    "                x_val, scaler_features, features_to_pass, train=False\n",
    "            )\n",
    "            x_test = utils_ml.scaler_pipeline(\n",
    "                x_test, scaler_features, features_to_pass, train=False\n",
    "            )\n",
    "\n",
    "            # create predictors for full time series\n",
    "            x_pred_temp = utils_ml.scaler_pipeline(\n",
    "                merged_df, scaler_features, features_to_pass, train=False\n",
    "            )\n",
    "\n",
    "            # Transform Y values\n",
    "            y_train = pd.DataFrame(\n",
    "                scaler_labels.fit_transform(y_train),\n",
    "                index=y_train.index,\n",
    "                columns=y_train.columns,\n",
    "            )\n",
    "            y_val = pd.DataFrame(\n",
    "                scaler_labels.transform(y_val),\n",
    "                index=y_val.index,\n",
    "                columns=y_val.columns,\n",
    "            )\n",
    "            y_test = pd.DataFrame(\n",
    "                scaler_labels.transform(y_test),\n",
    "                index=y_test.index,\n",
    "                columns=y_test.columns,\n",
    "            )\n",
    "\n",
    "            # Model Initialization\n",
    "            hidden_nodes = 50\n",
    "            opt = Adam(learning_rate=0.001)\n",
    "            model = Sequential()\n",
    "            model.add(\n",
    "                Dense(\n",
    "                    hidden_nodes,\n",
    "                    input_dim=x_train.shape[1],\n",
    "                    activation=\"relu\",\n",
    "                    use_bias=True,\n",
    "                    kernel_initializer=\"glorot_uniform\",\n",
    "                    kernel_regularizer=L2(l2=0.1),\n",
    "                )\n",
    "            )\n",
    "            model.add(Dropout(rate=0.2))\n",
    "            model.add(\n",
    "                Dense(\n",
    "                    2 * hidden_nodes,\n",
    "                    input_dim=x_train.shape[1],\n",
    "                    activation=\"relu\",\n",
    "                    use_bias=True,\n",
    "                    kernel_initializer=\"glorot_uniform\",\n",
    "                )\n",
    "            )\n",
    "            model.add(Dropout(rate=0.2))\n",
    "            model.add(Dense(1))\n",
    "            model.compile(optimizer=opt, loss=\"mse\", metrics=[RootMeanSquaredError()],)\n",
    "\n",
    "            # Hyper Paramter Adjustments\n",
    "            early_stopping = callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\", patience=5, min_delta=0.0, restore_best_weights=True\n",
    "            )\n",
    "\n",
    "            adaptive_lr = callbacks.ReduceLROnPlateau(\n",
    "                monitor=\"val_loss\", factor=0.1, min_lr=0\n",
    "            )\n",
    "\n",
    "            history = model.fit(\n",
    "                x_train,\n",
    "                y_train,\n",
    "                epochs=700,\n",
    "                validation_data=(x_val, y_val),\n",
    "                verbose=0,\n",
    "                callbacks=[early_stopping, adaptive_lr],\n",
    "            )\n",
    "\n",
    "            # Score and Tracking Metrics\n",
    "            y_train = pd.DataFrame(\n",
    "                scaler_labels.inverse_transform(y_train),\n",
    "                index=y_train.index,\n",
    "                columns=[\"Y Train\"],\n",
    "            ).sort_index(axis=0, ascending=True)\n",
    "            y_train_hat = pd.DataFrame(\n",
    "                scaler_labels.inverse_transform(model.predict(x_train)),\n",
    "                index=x_train.index,\n",
    "                columns=[\"Y Train Hat\"],\n",
    "            ).sort_index(axis=0, ascending=True)\n",
    "            y_val = pd.DataFrame(\n",
    "                scaler_labels.inverse_transform(y_val),\n",
    "                index=y_val.index,\n",
    "                columns=[\"Y Val\"],\n",
    "            ).sort_index(axis=0, ascending=True)\n",
    "            y_val_hat = pd.DataFrame(\n",
    "                scaler_labels.inverse_transform(model.predict(x_val)),\n",
    "                index=x_val.index,\n",
    "                columns=[\"Y Val Hat\"],\n",
    "            ).sort_index(axis=0, ascending=True)\n",
    "\n",
    "            train_points, val_points = [len(y_train)], [len(y_val)]\n",
    "\n",
    "            train_me = (sum(y_train_hat.values - y_train.values) / train_points).item()\n",
    "            train_rmse = mean_squared_error(\n",
    "                y_train.values, y_train_hat.values, squared=False\n",
    "            )\n",
    "            train_mae = mean_absolute_error(y_train.values, y_train_hat.values)\n",
    "\n",
    "            val_me = (sum(y_val_hat.values - y_val.values) / val_points).item()\n",
    "            val_rmse = mean_squared_error(y_val.values, y_val_hat.values, squared=False)\n",
    "            val_mae = mean_absolute_error(y_val.values, y_val_hat.values)\n",
    "\n",
    "            train_e = [train_me, train_rmse, train_mae]\n",
    "            val_e = [val_me, val_rmse, val_mae]\n",
    "\n",
    "            test_cols = [\"Test ME\", \"Test RMSE\", \"Test MAE\"]\n",
    "\n",
    "            train_errors = np.array([train_e + val_e]).reshape((1, 6))\n",
    "            errors_col = [\n",
    "                \"Train ME\",\n",
    "                \"Train RMSE\",\n",
    "                \"Train MAE\",\n",
    "                \"Validation ME\",\n",
    "                \"Validation RMSE\",\n",
    "                \"Validation MAE\",\n",
    "            ]\n",
    "            df_metrics = pd.DataFrame(\n",
    "                train_errors, index=([str(current_fold)]), columns=errors_col\n",
    "            )\n",
    "\n",
    "            df_metrics[\"Train Points\"] = train_points\n",
    "            df_metrics[\"Validation Points\"] = val_points\n",
    "            df_metrics[\"Train r2\"], _ = pearsonr(\n",
    "                y_train.values.flatten(),\n",
    "                y_train_hat.values.flatten(),\n",
    "            )\n",
    "            df_metrics[\"Validation r2\"], _ = pearsonr(\n",
    "                y_val.values.flatten(),\n",
    "                y_val_hat.values.flatten(),\n",
    "            )\n",
    "            temp_metrics = pd.concat(\n",
    "                objs=[\n",
    "                    temp_metrics,\n",
    "                    df_metrics,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Model Prediction\n",
    "            prediction_temp = pd.DataFrame(\n",
    "                scaler_labels.inverse_transform(model.predict(x_pred_temp)),\n",
    "                index=x_pred_temp.index,\n",
    "                columns=[current_fold],\n",
    "            )\n",
    "\n",
    "            # append prediction to model runs\n",
    "            model_runs = model_runs.join(prediction_temp, how=\"outer\")\n",
    "\n",
    "            # Test Sets and Plots\n",
    "            try:\n",
    "                y_test = pd.DataFrame(\n",
    "                    scaler_labels.inverse_transform(y_test),\n",
    "                    index=y_test.index,\n",
    "                    columns=[\"Y Test\"],\n",
    "                ).sort_index(axis=0, ascending=True)\n",
    "                y_test_hat = pd.DataFrame(\n",
    "                    scaler_labels.inverse_transform(model.predict(x_test)),\n",
    "                    index=y_test.index,\n",
    "                    columns=[\"Y Test Hat\"],\n",
    "                ).sort_index(axis=0, ascending=True)\n",
    "                test_points = len(y_test)\n",
    "                test_me = (sum(y_test_hat.values - y_test.values) / test_points).item()\n",
    "                test_rmse = mean_squared_error(\n",
    "                    y_test.values, y_test_hat.values, squared=False\n",
    "                )\n",
    "                test_mae = mean_absolute_error(y_test.values, y_test_hat.values)\n",
    "\n",
    "                test_errors = np.array([test_me, test_rmse, test_mae]).reshape((1, 3))\n",
    "                test_cols = [\"Test ME\", \"Test RMSE\", \"Test MAE\"]\n",
    "                test_metrics = pd.DataFrame(\n",
    "                    test_errors, index=[str(current_fold)], columns=test_cols\n",
    "                )\n",
    "                test_metrics[\"Test Points\"] = test_points\n",
    "                test_metrics[\"Test r2\"], _ = pearsonr(\n",
    "                    y_test.values.flatten(), y_test_hat.values.flatten()\n",
    "                )\n",
    "                temp_metrics.loc[\n",
    "                    str(current_fold), test_metrics.columns\n",
    "                ] = test_metrics.loc[str(current_fold)]\n",
    "                plot_kfolds = True\n",
    "\n",
    "            except:\n",
    "                temp_metrics.loc[\n",
    "                    str(current_fold), [\"Test ME\", \"Test RMSE\", \"Test MAE\"]\n",
    "                ] = np.NAN\n",
    "                temp_metrics.loc[str(current_fold), \"Test Points\"] = 0\n",
    "                temp_metrics.loc[str(current_fold), \"Test r2\"] = np.NAN\n",
    "\n",
    "            current_fold += 1\n",
    "            n_epochs.append(len(history.history[\"loss\"]))\n",
    "        logging.info(f\"Finished k-fold cross validation for well: {well_id}\")\n",
    "        logging.info(f\"Starting model training for well: {well_id}\")\n",
    "        epochs = int(sum(n_epochs) / n_folds)\n",
    "\n",
    "        # Reset feature scalers\n",
    "        x, scaler_features = utils_ml.scaler_pipeline(\n",
    "            x, scaler_features, features_to_pass, train=True\n",
    "        )\n",
    "        x_pred = utils_ml.scaler_pipeline(\n",
    "            merged_df, scaler_features, features_to_pass, train=False\n",
    "        )\n",
    "        y = pd.DataFrame(\n",
    "            scaler_labels.fit_transform(y), index=y.index, columns=y.columns\n",
    "        )\n",
    "\n",
    "        # Retrain Model with number of epochs\n",
    "        history = model.fit(x, y, epochs=epochs, verbose=0)\n",
    "        metrics_avg = pd.DataFrame(temp_metrics.mean(), columns=[well_id]).transpose()\n",
    "        summary_metrics_df = pd.concat(objs=[summary_metrics_df, metrics_avg])\n",
    "\n",
    "        # Model Prediction\n",
    "        prediction = pd.DataFrame(\n",
    "            scaler_labels.inverse_transform(model.predict(x_pred)).astype(float),\n",
    "            index=x_pred.index,\n",
    "            columns=[well_id],\n",
    "        )\n",
    "        model_runs = model_runs.join(prediction, how=\"outer\")\n",
    "        data_dict_well[\"runs\"][well_id] = model_runs\n",
    "        spread = pd.DataFrame(index=prediction.index, columns=[\"mean\", \"std\"])\n",
    "        spread[\"mean\"] = model_runs.mean(axis=1)\n",
    "        spread[\"std\"] = model_runs.std(axis=1)\n",
    "        comp_r2 = r2_score(\n",
    "            scaler_labels.inverse_transform(y.values.reshape(-1, 1)),\n",
    "            scaler_labels.inverse_transform(model.predict(x)),\n",
    "        )\n",
    "        summary_metrics_df.loc[well_id, \"Comp R2\"] = comp_r2\n",
    "        # Data Filling\n",
    "        gap_time_series = pd.DataFrame(\n",
    "            data_dict_well[\"timeseries\"][well_id], index=prediction.index\n",
    "        )\n",
    "        filled_time_series = gap_time_series[well_id].fillna(prediction[well_id])\n",
    "        if y_raw.dropna().index[-1] > prediction.index[-1]:\n",
    "            filled_time_series = pd.concat(\n",
    "                [filled_time_series, y_raw.dropna()], join=\"outer\", axis=1\n",
    "            )\n",
    "            filled_time_series = filled_time_series.iloc[:, 0]\n",
    "            filled_time_series = filled_time_series.fillna(y_raw)\n",
    "        imputation_df = imputation_df.join(filled_time_series, how=\"outer\")\n",
    "        model_output_df = model_output_df.join(prediction, how=\"outer\")\n",
    "        logging.info(f\"Finished model training for well: {well_id}\")\n",
    "    except (\n",
    "        ValueError,\n",
    "        TypeError,\n",
    "        KeyError,\n",
    "        IndexError,\n",
    "        FileNotFoundError,\n",
    "        PermissionError,\n",
    "        ConnectionError,\n",
    "        Exception,\n",
    "    ) as error:\n",
    "        logging.error(f\"An exception ocurred on well: {well_id}\\n{error}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        continue\n",
    "# well_data_dict['Data_Smooth'] = imp.smooth(Imputed_Data.loc[imputation_range], Well_Data['Data'], window = 18)\n",
    "data_dict_well[\"Data\"] = imputation_df.loc[imputation_range]\n",
    "# well_data_dict['Raw_Output'] = Model_Output.loc[imputation_range]\n",
    "data_dict_well[\"Metrics\"] = summary_metrics_df\n",
    "summary_metrics_df.to_csv(\n",
    "    os.path.join(\n",
    "        project_args.dataset_outputs_dir,\n",
    "        f\"metrics_0{project_args.iteration_current}.csv\",\n",
    "    ),\n",
    "    index=True,\n",
    ")\n",
    "utils.save_pickle(\n",
    "    data_dict_well,\n",
    "    f\"{project_args.aquifer_name}_0{project_args.iteration_current}.pickle\",\n",
    "    project_args.dataset_outputs_dir,\n",
    ")\n",
    "# utils_ml.Aquifer_Plot(Well_Data['Data'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the prior along with the raw data\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.scatter(y_data.index, y_data, label=\"Raw Data\")\n",
    "ax.plot(prior, label=\"Prior\")\n",
    "ax.legend()\n",
    "ax.set_title(f\"Well: {well_id}\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Water Level\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groundwater_imp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
