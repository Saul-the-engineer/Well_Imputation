# -*- coding: utf-8 -*-
"""
Created on Thu Jan 21 23:23:13 2021

@author: saulg
"""
import pandas as pd
import numpy as np
import datetime as dt
import matplotlib.pyplot as plt
import pickle
import os
from scipy import interpolate
from sklearn.metrics import mean_squared_error
import gc

class imputation():
    def __init__(self, data_root ='./Datasets', figures_root = './Figures Imputed'):
        # Data Path
        if os.path.isdir(data_root) is False:
            os.makedirs(data_root)
            print('The dataset folder with data is empty')
        self.data_root = data_root
        
        # Fquifer Root is the location to save figures.
        if os.path.isdir(figures_root) is False:
            os.makedirs(figures_root)
        self.figures_root = figures_root
        return
       
    def read_pickle(self, pickle_file, pickle_root):
        wellfile = pickle_root + pickle_file + '.pickle'
        with open(wellfile, 'rb') as handle:
            wells = pickle.load(handle)
        return wells
    
    def Save_Pickle(self, Data, name:str, path:str, protocol:int = 3):
        with open(path + '/' + name + '.pickle', 'wb') as handle:
            pickle.dump(Data, handle, protocol=protocol)
            
    def log_errors(self, errors, name:str, path:str):
        if len(errors) == 0: pass
        else:
            with open(path + '/' + name + '.txt', 'w') as output:
                for row in errors:
                    output.write(str(row) + '\n')
    
    def Data_Split(self, Data, well_name_temp, Shuffle=False):
        if Shuffle:
            # The frac keyword argument specifies the fraction of rows
            # to return in the random sample
             Data = Data.sample(frac=1)
        Y = Data[well_name_temp].to_frame()
        X = Data.drop(well_name_temp, axis=1)
        return Y, X
    
    def Data_Join(self, pd1, pd2, method='outer', axis=1):
        return pd.concat([pd1, pd2], join = method, axis=1)
    
    def test_range_split(self, df, f_index, name, min_points = 1, 
                         cut_left= None, gap_year=None, random=True, 
                          seed_start = 42, max_tries = 15, max_gap = 5):
        self.gap_year = gap_year
        self.cut_left = cut_left
        attempt = 0
        Y_Test = []
        Y_Train = []
        while attempt <= max_tries and len(Y_Test) <= min_points and len(Y_Train) <= 3:
            if self.gap_year == None: 
                gap_year = np.random.randint(1, max_gap+1)
            if self.cut_left == None: 
                cut_left = None
            cut_left, cut_right = self.define_gap(df, f_index, cut_left, gap_year, 
                                                  seed = seed_start + attempt, 
                                                  random = random)
            cut_left_index = dt.datetime(cut_left, 1, 1)
            cut_right_index = dt.datetime(cut_right, 1, 1)
            Y_Test = df[(df.index >= cut_left_index) & 
                    (df.index < cut_right_index)].dropna() 
            Y_Train = df[(df.index < cut_left_index) |
                    (df.index >= cut_right_index)].dropna() 
            attempt += 1
        if attempt == max_tries:
            self.cut_left, self.cut_right= [f_index[0], f_index[0]]
            error = f'{name} could not be split into train/test sets'
            return df.dropna(), pd.DataFrame(), error
        self.cut_left, self.cut_right= [cut_left, cut_right]
        return Y_Train, Y_Test, None
    
    def define_gap(self, df, f_index, cut_left, gap_year, seed, random=True):
        np.random.seed(seed)
        df = df.dropna()
        if random == True and cut_left == None:
            if df.index[0].year < f_index[0].year: date_min = f_index[0].year
            else: date_min = df.index[0].year
            if df.index[-1].year > f_index[-1].year: cut_range_end = f_index[-1].year
            else: cut_range_end = df.index[-1].year
            
            df_index = df[(df.index >= dt.datetime(date_min, 1, 1)) &
                          (df.index <= dt.datetime(cut_range_end - gap_year, 1, 1))]
            cut_left = df.index[np.random.randint(0, len(df_index))].year
            cut_right = cut_left + gap_year
        elif random == False and cut_left != None:
            cut_right = cut_left + gap_year  
        else: cut_left, cut_right = [1995, 2001] 
        return cut_left, cut_right
    
    def interpolate(self, feature_index, y, name, shift = 60):
        self.shift_rw_max = shift
        index_check = y.dropna()
        startdt = pd.to_datetime(index_check.index[0])
        enddt   = pd.to_datetime(feature_index[0])
        lag = len(pd.date_range(start=startdt,end=enddt,freq='M'))
        if lag > 0: y = index_check
        int_y, x_int_index = self._interpolate_integers(lag, feature_index, y)
        
        p_class = interpolate.pchip(int_y.index, int_y[name], extrapolate=False)
        pchip_index = feature_index.shift(shift-1, freq='MS').union(feature_index.shift(-shift, freq='MS'))
        pchip_int   = np.arange(-shift, len(pchip_index)-shift, 1).astype(int)
        pchip_int   = pd.DataFrame(pchip_int, index = pchip_index, columns = ['x'])
        interp  = p_class(pchip_index[:].astype('int'))
        pchip   = pd.DataFrame(interp, index=pchip_index[:], columns=['pchip'])
        x_int_index = self.Data_Join(x_int_index, pchip)
        x_int_index = x_int_index.drop('pchip', axis=1)
        x_int_index = x_int_index['x'].fillna(pchip_int['x'])
        return pchip, x_int_index, pchip_int
    
    def _interpolate_integers(self, lag, feature_index, data):
        data = data.dropna()
        start = min(feature_index[0], data.index[0])
        end = max(feature_index[-1], data.index[-1])
        data_range = pd.date_range(start= start, end = end, freq='MS')
        int_values = np.arange(-lag, len(data_range)-lag, 1).astype(int)
        x_index    = pd.Series(int_values, index = data_range, name = 'x')
        int_y      = pd.concat([x_index, data], axis=1).dropna()
        return int_y, x_index
    
    def linear_extrap(self, f_index, y, shift, reg_perc = [1.0, 0.5, 0.25, 0.10], max_sd = 3 , outlier = 3):
        # Generate extrapolation df for left and right sides
        left = pd.DataFrame(index = f_index.index, columns = reg_perc)
        right = pd.DataFrame(index = f_index.index, columns = reg_perc)
        # Add df to dict to be iteratable 
        d_side = {"left":left, "right":right}
        # Dictionary to store metadata about slope corrections
        d_slope = {}
        # Find smallest slope percentage to save index for linear regression
        # and data to determine y-intercept
        s_min = min(reg_perc)
        
        # High-Level: Generate slope metadata df with S_raw, Int, Mean, Slope
        for i, side in enumerate(d_side):
            slope_df = pd.DataFrame(index = reg_perc, columns = ['Slope', 'Int', 'Mean'])
            for j, perc in enumerate(reg_perc):
                points = int(len(y.dropna()) * perc)
                
                # Obtain the percentage of points in each side
                # Grab all indecies associated with the data range
                # reindex data so that regression makes sense
                if side == "left":
                    data = y[:points]
                    index = f_index[f_index.index <= data.index[-1]]
                    index = pd.DataFrame(np.arange(points - len(index), points, 1), index = index.index, columns = ['x'])
                    if perc == s_min:
                        index_l = index
                
                elif side == "right":
                    data = y[-points:]
                    index = f_index[f_index.index >= data.index[0]]
                    index = pd.DataFrame(np.arange(len(index) - len(index), len(index), 1), index = index.index, columns = ['x'])
                    if perc == s_min:
                        index_r = index

                # Get sample Mean and Standard Deviation
                mean = data['pchip'].mean()
                sd = data['pchip'].std()
                # Remove outliers for linear regression
                data = data[(data['pchip'] <= mean + outlier * sd)]
                # Perform Linear Regression
                reg = self.linear_regression(index, data)
                # Unpack slope and intercept
                slope, intercept, _ = reg
                # Get Mean of data set once outliers are removed
                mean = data['pchip'].mean()
                # Save slope mean and intercept
                slope_df.loc[perc] = np.array([slope, intercept, mean])
                # Extrapolate based on slope store extrapolation
                d_side[side][perc] = index * slope + intercept
                
            # Update datafrme in dictionary
            d_slope[side] = slope_df
            # Get average slope for each side
            if side == 'left': slope_l = slope_df['Slope'].mean()
            elif side == 'right': slope_r = slope_df['Slope'].mean()
        
        # Calculate population Mean, Std, Max value, Min Value
        pop_mean = y['pchip'].mean()
        pop_std = y['pchip'].std()
        max_value = pop_mean + max_sd * pop_std
        min_value = pop_mean - max_sd * pop_std
        
        # Extrapolate based on mean slope
        extrap_l = index_l * slope_l + d_slope['left']['Mean'].loc[s_min]
        extrap_r = index_r * slope_r + d_slope['right']['Mean'].loc[s_min]
        
        # Replace extrapolated values that exceed limits
        # Correct Left Side
        extrap_l['x'] = np.where(extrap_l['x'] <= max_value, extrap_l['x'], max_value)
        extrap_l['x'] = np.where(extrap_l['x'] >= min_value, extrap_l['x'], min_value)
        
        # Correct Right Side
        extrap_r['x'] = np.where(extrap_r['x'] <= max_value, extrap_r['x'], max_value)
        extrap_r['x'] = np.where(extrap_r['x'] >= min_value, extrap_r['x'], min_value)
        
        # Fill Extrapolation
        extrap_df = self.Data_Join(f_index, y).drop(['x'], axis=1)
        filled = extrap_df['pchip'].fillna(extrap_l['x'])
        filled = filled.fillna(extrap_r['x'])
        return filled, d_side, d_slope

    def hampel_filter(self, df_imp, df_obs, max_sd  = 3, window = 36, center = True):
        '''
        adapted from hampel function in R package pracma
        x = 1-d numpy array of numbers to be filtered
        k = number of items in window/2 (# forward and backward wanted to capture in median filter)
        max_sd = number of standard deviations to use; 3 is default
        L is Limit sigma ~ 1.4826 mad (mean absolute deviation)
        Threshold is therefore max_sd * L * mad
        "Hampel F. R., ”The influence curve and its role in robust estimation,” 
        Journal of the American Statistical Association, 69, 382–393, 1974."
        '''
        # Create empty df same size as imputation df
        df = pd.DataFrame(index = df_imp.index, columns = df_imp.columns)
        # Filter observed values within data range
        df_obs = df_obs.loc[df_imp.index[df_imp.index <= df_obs.index[-1]], :]
        # Fill df to ensure hampel doesn't remove observed measurements
        df = df.fillna(df_obs)
        
        # Standard deviation approximation
        L = 1.4826
        
        # Copy data
        data = df_imp.copy()
        
        # Generate find average median Done because edges will not have data
        if center == True:
            roll = data.rolling(window=window, min_periods = 1, center=True).median()
        elif center == False:
            roll = data.rolling(window=window, min_periods = 1, center=False).median()
        
        # Applying filter
        difference = np.abs(roll - data)
        mad = difference.rolling(window, min_periods = 1).median()
        threshold = max_sd * L * mad
        outlier_idx = difference > threshold
        data[outlier_idx] = roll # np.nan #roll_median #
        
        # Fill observed data with filtered measurements
        df = df.fillna(data).astype(float)
        '''
        # Plot outlier removal
        cols = df_imp.columns.to_list()
        for col in cols:
            df_obs[col].plot()
            plt.show()
            df_imp[col].plot()
            plt.show()
            df[col].plot()
            plt.show()
            print('Done.')
         '''
        return df
    
    def smooth(self, df_imp, df_obs, window = 36, center = True):
        # Create empty df same size as imputation df
        df = pd.DataFrame(index = df_imp.index, columns = df_imp.columns)
        # Filter observed values within data range
        df_obs = df_obs.loc[df_imp.index[df_imp.index <= df_obs.index[-1]], :]
        # Fill df to ensure hampel doesn't remove observed measurements
        df = df.fillna(df_obs)
        # Copy data
        data = df_imp.copy()
        # Generate find average median Done because edges will not have data
        if center == True:
            roll = data.rolling(window=window, min_periods = 1, center=True).median()
        elif center == False:
            roll = data.rolling(window=window, min_periods = 1, center=False).median()
        # Fill observed data with filtered measurements
        df = df.fillna(roll).astype(float)
        return df
        
    
    def linear_regression(self, f_index, y):
        df = self.Data_Join(f_index, y).dropna()
        x = df['x'].values
        N = len(x)
        y = df['pchip'].values
        one = np.ones((N))
        if len(x) == len(y) and len(x) >= 3:
            A      = np.vstack([x,one]).T
            B0, B1 = np.linalg.lstsq(A, y)[0]
        reg_line = 'y = {}X + {}'.format(B0, B1)
        return (B0, B1, reg_line)
    
    def rolling_windows(self, df, windows = [3, 6]):
        rw_dict = dict()
        for i, months in enumerate(windows):
            key = str(months) + 'm_rw'
            rw = df.rolling(months, center=True, min_periods = 1).mean()
            rw_dict[key] = rw
        rw = pd.DataFrame.from_dict(rw_dict)
        return rw
        
    def scaler_pipline(self, x, scaler, ns_names, train=False):
        if train == True:
            x_scale = x.drop(ns_names, axis=1)
            x_temp = scaler.fit_transform(x_scale)
            x_temp = pd.DataFrame(x_temp, index = x_scale.index, columns = x_scale.columns)
            X = self.Data_Join(x_temp, x[ns_names], method='inner')
            return [X, scaler]
            
        else:
            x_scale = x.drop(ns_names, axis=1)
            x_temp = scaler.transform(x_scale)
            x_temp = pd.DataFrame(x_temp, index = x_scale.index, columns = x_scale.columns)
            X = self.Data_Join(x_temp, x[ns_names], method='inner')
            return X
    
    def feature_correlation(self, df, Feature_Data, raw, score):
        data_clean = Feature_Data.dropna()
        fi = len(Feature_Data)
        wi = len(data_clean)
        data_zc = data_clean - data_clean.mean()
        index = Feature_Data.columns[0]
        df.loc[index, 'FI'] = fi
        df.loc[index, 'WI'] = wi
        names = Feature_Data.columns[1:].tolist()
        score = score['w_score'].tolist()
        s_list = list(zip(names, score))
        s_list.sort(reverse=True, key = lambda x: x[1])
        for i, obj in enumerate(s_list):
            name, r = obj
            if raw.index[-1] > Feature_Data.index[-1]: index_mask = Feature_Data.index
            else: index_mask = pd.date_range(Feature_Data.index[0], raw.index[-1], freq='MS')
            fip = raw[name].loc[index_mask].count()/fi
            wip = raw[name].loc[Feature_Data.dropna().index].count()/wi
            rmse  = mean_squared_error(data_zc[index].values, data_zc[name].values, squared=False)
            output = [name, r, fip, wip, rmse]
            col_names = [f'F{i}', f'F{i} w_r2', f'F{i} fip', f'F{i} wip', f'F{i} nRMSE']
            if col_names[0] not in df.columns:
                df[col_names] = np.nan
            df.loc[index, col_names] = output
        return df
    
    def metrics(self, Metrics, n_wells):
        metrics_result = Metrics.sum(axis = 0)
        normalized_metrics = metrics_result/n_wells
        with open(self.figures_root + 'Error_Aquifer.txt', "w") as outfile:
            print('Raw Model Metrics:  \n' + str(metrics_result), file=outfile)
            print('\n Normalized Model Metrics Per Well:  \n' + str(normalized_metrics), file=outfile)
        np.savetxt(self.figures_root + 'Error_Wells.txt', Metrics.values, fmt='%d')
        return metrics_result, normalized_metrics

    def Model_Training_Metrics_plot(self, Data, name, show=False):
        fig = plt.figure()
        for key in Data: plt.plot(Data[key])
        plt.grid(True)
        plt.gca().set_ylim(-0.5, 5)
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(Data.keys())
        plt.savefig(self.figures_root + '/' + name + '_Training_History')
        if show: plt.show()
        else:
            fig.clf()
            plt.close(fig)
        gc.collect()
    
    def trend_plot(self, pchip, extrap_df, extrap_md, raw, name, extension = '.png', show=False):
        slopes_l = extrap_df['left']
        slopes_r = extrap_df['right']
        meta_l = extrap_md['left']
        meta_r = extrap_md['right']
        num_plots = len(slopes_l.columns)
        
        fig = plt.figure(figsize=(12, 8))
        ax1 = fig.add_subplot(111)
        
        colormap = plt.cm.nipy_spectral
        colors = [colormap(i) for i in np.linspace(0.3, 0.9, num_plots)]
        ax1.set_prop_cycle('color', colors)

        for i in range(num_plots):
            ax1.plot(slopes_l.index, slopes_l.iloc[:,i])
        ax1.plot(pchip.index, pchip, color = "black")
        ax1.scatter(raw.index, raw, s= 3, c= 'red')
        ax1.legend(slopes_l.columns.tolist() + ['Prior', 'Observations'], title = 'Total Data Percentage')
        for i in range(num_plots):
            ax1.plot(slopes_r.index, slopes_r.iloc[:,i])
        ax1.set_title('Prior')
        ax1.set_ylabel('Groundwater Level')
        ax1.text(x=0.05, y=-0.15, s = meta_l.to_string(index=True, float_format = "{0:.5}".format),
                  fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax1.transAxes)
        ax1.text(x=-0.05, y=-0.08, s = 'Left Percentage',
                  fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax1.transAxes)
        ax1.text(x=0.60, y=-0.15, s = meta_r.to_string(index=True, float_format = "{0:.5}".format),
                  fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax1.transAxes)
        ax1.text(x=0.48, y=-0.08, s = 'Right Percentage',
                  fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax1.transAxes)
        plt.tight_layout()
        fig.savefig(self.figures_root + '/' + name + '_00_Trend' + extension)
        if show: plt.show(fig)
        else:
            fig.clf()
            plt.close(fig)
        gc.collect()

    def rw_plot(self, y, rw, name, save = False, extension = '.png', show=False):
        fig = plt.figure(figsize=(12, 8))
        plt.plot(rw)
        plt.scatter(y.index, y, s=3, c = 'black')
        plt.ylabel('Groundwater Level')
        plt.legend(rw.columns.tolist() + ['Observations'])
        plt.title('Long-Term Trends: ' + name)
        if save: plt.savefig(self.figures_root + '/' + name + '_00_RW' + extension)
        if show: plt.show()
        else:
            fig.clf()
            plt.close(fig)
        gc.collect()
    
    def Q_Q_plot(self, Prediction, Observation, name, limit_low = 0, limit_high = 1, extension = '.png', show=False):
        #Plotting Prediction Correlation
        fig = plt.figure()
        ax1 = fig.add_subplot(111)
        plt.scatter(Prediction, Observation)
        plt.ylabel('Observation')
        plt.xlabel('Prediction')
        plt.legend(['Prediction', 'Observation'])
        plt.title('Prediction Correlation: ' + name)
        cor_line_x = np.linspace(limit_low, limit_high, 9)
        cor_line_y = cor_line_x
        plt.xlim(limit_low, limit_high)
        plt.ylim(limit_low, limit_high)
        plt.plot(cor_line_x, cor_line_y, color='r')
        ax1.set_aspect('equal', adjustable='box')
        plt.savefig(self.figures_root + '/' + name + '_01_Q_Q' + extension)
        if show: plt.show()
        else:
            fig.clf()
            plt.close(fig)
        gc.collect()

    def observeation_vs_prediction_plot(self, Prediction_X, Prediction_Y, Observation_X, Observation_Y, name, metrics=None, error_on = False, show=False):
        fig = plt.figure(figsize=(12, 8))
        ax = fig.add_subplot(111)
        ax.plot(Prediction_X, Prediction_Y, "darkblue")
        # Potential bug # pandas version 1.4 pandas.errors.InvalidIndexError: (slice(None, None, None), None)
        ax.plot(Observation_X, Observation_Y, label= 'Observations', color='darkorange')
        ax.set_ylabel('Groundwater Level')
        ax.legend(['Prediction', 'Observation'])
        ax.set_title('Observation Vs Prediction: ' + name)
        if error_on:
          ax.text(x=0.0, y=-0.15, s = metrics[['Train ME','Train RMSE', 'Train MAE', 'Train r2']].to_string(index=True, float_format = "{0:.3}".format),
                  fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)
          ax.text(x=0.25, y=-0.15, s = metrics[['Validation ME','Validation RMSE', 'Validation MAE', 'Validation r2']].to_string(index=True, float_format = "{0:.3}".format),
                  fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)          
          extent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())
          fig.savefig(self.figures_root  + '/' + name + '_02_Observation', bbox_inches=extent.expanded(1.1, 1.6))
        else: fig.savefig(self.figures_root  + '/' + name + '_02_Observation')
        if show: plt.show()
        else:
            fig.clf()
            plt.close(fig)
        gc.collect()
        
    def residual_plot(self, Prediction_X, Prediction_Y, Observation_X, Observation_Y, name, show=False):
        date_rng = pd.DataFrame(np.arange(0, len(Observation_X), 1), index = Observation_X)
        data = self.Data_Join(Prediction_Y, Observation_Y).dropna()
        data = self.Data_Join(data, date_rng).dropna()
        data.columns = ['Prediction_Y', 'Observation_Y', 'Spline Index']
        residual = data['Prediction_Y'] - data['Observation_Y']
        spline = interpolate.UnivariateSpline(data['Spline Index'], residual, s=1000000)
        
        fig, axs = plt.subplots(2, 1, figsize = (12,12))
        axs[0].plot(data.index, residual, marker = 'o', linestyle='None', markersize=5, color = "black")
        axs[0].plot(Observation_X, np.zeros(shape = (len(Observation_X), 1)), color = 'royalblue', linewidth= 2.0)
        axs[0].plot(data.index, spline(data['Spline Index']), color = "red", linewidth= 3.0)
        axs[0].legend(['Residual', 'Zero', 'Spline Trend'])
        
        axs[0].set_ylabel('Prediction Residual Error')
        axs[0].set_title('Residual Error: ' + name)
        
        axs[1].plot(Observation_X, Observation_Y, marker = 'o', linestyle='None', markersize=5, color = "black")
        axs[1].plot(Observation_X, Observation_Y[name].mean()*np.ones(shape = (len(Observation_X), 1)), color = 'royalblue', linewidth= 2.0)
        axs[1].legend(['Observations', 'Mean'])
        axs[1].set_ylabel('Groundwater Level')
        axs[1].set_title('Groundwater Observations: ' + name)
        
        plt.savefig(self.figures_root  + '/' + name + '_03_Residual_Plot')
        if show: plt.show()
        else:
            fig.clf()
            plt.close(fig)  
        gc.collect()

    def observeation_vs_imputation_plot(self, Prediction_X, Prediction_Y, Observation_X, Observation_Y, name, show=False):
        fig = plt.figure(figsize=(12, 8))
        plt.plot(Prediction_X, Prediction_Y, "darkblue")
        plt.plot(Observation_X, Observation_Y, label= 'Observations', color='darkorange')
        plt.ylabel('Groundwater Level')
        plt.xlabel('Date')
        plt.legend(['Imputed Values', 'Smoothed Observations'])
        plt.title('Observation Vs Imputation: ' + name)
        plt.savefig(self.figures_root  + '/' + name + '_04_Imputation')
        if show: plt.show()
        else: 
            fig.clf()
            plt.close(fig)
        gc.collect()

    def raw_observation_vs_prediction(self, Prediction, Raw, name, Aquifer, metrics=None, error_on = False, test=False, show=False):
        fig = plt.figure(figsize=(12, 8))
        ax = fig.add_subplot(111)
        ax.plot(Prediction.index, Prediction, 'darkblue', label='Prediction', linewidth=1.0)
        ax.scatter(Raw.index, Raw, color='darkorange', marker = '*', s=10, label= 'Observations')
        ax.set_title(Aquifer + ': ' + 'Well: ' + name + ' Predicted Values')
        ax.set_ylabel('Groundwater Level')
        ax.legend(fontsize = 'medium')
        if error_on:
          ax.text(x=0.0, y=-0.15, s = metrics[['Train ME','Train RMSE', 'Train MAE', 'Train r2']].to_string(index=True, float_format = "{0:.3}".format),
                  fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)
          ax.text(x=0.25, y=-0.15, s = metrics[['Validation ME','Validation RMSE', 'Validation MAE', 'Validation r2']].to_string(index=True, float_format = "{0:.3}".format),
                  fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)          
          extent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())
          if test:
               ax.text(x=0.5, y=-0.15, s = metrics[['Test ME','Test RMSE', 'Test MAE', 'Test r2']].to_string(index=True, float_format = "{0:.3}".format),
                  fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)
          fig.savefig(self.figures_root  + '/' + name + '_05_Prediction_vs_Raw', bbox_inches=extent.expanded(1.3, 1.6))
        else: fig.savefig(self.figures_root  + '/' + name + '_05_Prediction_vs_Raw')
        if show: plt.show()
        else:
            fig.clf()
            plt.close(fig)
        gc.collect()
        
    def raw_observation_vs_filled(self, Prediction, Raw, name, Aquifer, metrics=None, error_on = False, test=False, show=False):
        fig = plt.figure(figsize=(12, 8))
        ax = fig.add_subplot(111)
        ax.plot(Prediction.index, Prediction, 'darkblue', label='Prediction', linewidth=1.0)
        ax.plot(Raw.index, Raw, color='darkorange', label= 'Observations', linewidth=1.0)
        ax.set_title(Aquifer + ': ' + 'Well: ' + name + ' Imputed Values')
        ax.set_ylabel('Groundwater Level')
        ax.legend(fontsize = 'medium')
        if error_on:
          ax.text(x=0.0, y=-0.15, s = metrics[['Train ME','Train RMSE', 'Train MAE', 'Train r2']].to_string(index=True, float_format = "{0:.3}".format),
                  fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)
          ax.text(x=0.25, y=-0.15, s = metrics[['Validation ME','Validation RMSE', 'Validation MAE', 'Validation r2']].to_string(index=True, float_format = "{0:.3}".format),
                  fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)          
          extent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())
          if test:
               ax.text(x=0.5, y=-0.15, s = metrics[['Test ME','Test RMSE', 'Test MAE', 'Test r2']].to_string(index=True, float_format = "{0:.3}".format),
                  fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)
          fig.savefig(self.figures_root  + '/' + name + '_05_Filled_vs_Raw', bbox_inches=extent.expanded(1.3, 1.6))
        else: fig.savefig(self.figures_root  + '/' + name + '_05_Filled_vs_Raw')
        if show: plt.show()
        else:
            fig.clf()
            plt.close(fig)
        gc.collect()
    
    def raw_observation_vs_imputation(self, Prediction, Raw, name, Aquifer, metrics=None, show=False):
        fig = plt.figure(figsize=(12, 8))
        plt.plot(Prediction.index, Prediction, 'darkblue', label='Model', linewidth=1.0)
        plt.scatter(Raw.index, Raw, color='darkorange', marker = '*', s=10, label= 'Observations')
        plt.title(Aquifer + ': ' + 'Well: ' + name + ' Raw vs Model')
        plt.legend(fontsize = 'medium')
        plt.savefig(self.figures_root  + '/' + name + '_06_Imputation_vs_Raw')
        if show: plt.show()
        else:
            fig.clf()
            plt.close(fig)
        gc.collect()

    def observeation_vs_prediction_scatter_plot(self, Prediction, Y_train, Y_val, name, metrics=None, error_on = False, show=False):
        fig = plt.figure(figsize=(12, 8))
        ax = fig.add_subplot(111)
        ax.plot(Prediction.index, Prediction, "darkblue", linewidth=1.0)
        ax.scatter(Y_train.index, Y_train, color='darkorange', marker='*', s=10)
        ax.scatter(Y_val.index, Y_val, color='lightgreen', s=10)  
        ax.legend(['Prediction', 'Training Data', 'Validation Data'])
        ax.set_ylabel('Groundwater Level')
        ax.set_title('Observation Vs Prediction: ' + name)
        if error_on:
          ax.text(x=0.0, y=-0.15, s = metrics[['Train ME','Train RMSE', 'Train MAE', 'Train r2']].to_string(index=True, float_format = "{0:.3}".format),
                  fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)
          ax.text(x=0.25, y=-0.15, s = metrics[['Validation ME','Validation RMSE', 'Validation MAE', 'Validation r2']].to_string(index=True, float_format = "{0:.3}".format),
                  fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)          
          extent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())
          fig.savefig(self.figures_root  + '/' + name + '_07_Observation', bbox_inches=extent.expanded(1.2, 1.6))
        else: fig.savefig(self.figures_root  + '/' + name + '_07_Observation')
        if show: plt.show()
        else:
            fig.clf()
            plt.close(fig)
        gc.collect()
    
    def prediction_vs_test(self, Prediction, Well_set_original, y_test, name, metrics=None, error_on = False, show=False):
        fig = plt.figure(figsize=(12, 8))
        ax = fig.add_subplot(111)
        ax.plot(Prediction.index, Prediction, "darkblue", linewidth=1.0)
        ax.scatter(Well_set_original.index, Well_set_original, color='darkorange', marker='*', s=10)
        ax.scatter(y_test.index, y_test, color='lightgreen', s=10)
        ax.set_ylabel('Groundwater Level')
        ax.legend(['Prediction', 'Training Data', 'Test Data'])
        ax.set_title('Observation Vs Prediction: ' + name)
        ax.axvline(dt.datetime(int(self.cut_left), 1, 1), linewidth=0.25)
        ax.axvline(dt.datetime(int(self.cut_right), 1, 1), linewidth=0.25)
        if error_on:
          ax.text(x=0.0, y=-0.15, s = metrics[['Train ME','Train RMSE', 'Train MAE', 'Train r2']].to_string(index=True, float_format = "{0:.3}".format),
                  fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)
          ax.text(x=0.25, y=-0.15, s = metrics[['Validation ME','Validation RMSE', 'Validation MAE', 'Validation r2']].to_string(index=True, float_format = "{0:.3}".format),
                  fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)  
          ax.text(x=0.5, y=-0.15, s = metrics[['Test ME','Test RMSE', 'Test MAE', 'Test r2']].to_string(index=True, float_format = "{0:.3}".format),
                  fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)
          extent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())
          fig.savefig(self.figures_root  + '/' + name + '_08_Test', bbox_inches=extent.expanded(1.2, 1.6))
        else: fig.savefig(self.figures_root  + '/' + name + '_08_Test')
        if show: plt.show()
        else:
            fig.clf()
            plt.close(fig)
        gc.collect()
        
    def prediction_kfold(self, Prediction, Well_set_original, y_test, name, metrics=None, error_on = False, show=False, plot=False):
        if plot == True:
            fig = plt.figure(figsize=(12, 8))
            ax = fig.add_subplot(111)
            ax.plot(Prediction.index, Prediction, "darkblue", linewidth=1.0)
            ax.scatter(Well_set_original.index, Well_set_original, color='darkorange', marker='*', s=10)
            ax.scatter(y_test.index, y_test, color='lightgreen', s=10)
            ax.set_ylabel('Groundwater Level')
            ax.legend(['Prediction', 'Training Data', 'Test Data'])
            ax.set_title('Observation Vs Prediction: ' + name)
            ax.axvline(y_test.index[0], linewidth=0.25)
            ax.axvline(y_test.index[-1], linewidth=0.25)
            if error_on:
              ax.text(x=0.0, y=-0.15, s = metrics[['Train ME','Train RMSE', 'Train MAE', 'Train r2']].to_string(index=True, float_format = "{0:.3}".format),
                      fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)
              ax.text(x=0.25, y=-0.15, s = metrics[['Validation ME','Validation RMSE', 'Validation MAE', 'Validation r2']].to_string(index=True, float_format = "{0:.3}".format),
                      fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)  
              ax.text(x=0.5, y=-0.15, s = metrics[['Test ME','Test RMSE', 'Test MAE', 'Test r2']].to_string(index=True, float_format = "{0:.3}".format),
                      fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)
              extent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())
              fig.savefig(self.figures_root  + '/' + name + '_08_kfold', bbox_inches=extent.expanded(1.2, 1.6))
            else: fig.savefig(self.figures_root  + '/' + name + '_08_kfold')
            if show: plt.show()
            else:
                fig.clf()
                plt.close(fig)
            gc.collect()
        
    def prediction_vs_test_kfold(self, Prediction, Well_set_original, name, metrics=None, error_on = False, show=False):
        fig = plt.figure(figsize=(12, 8))
        ax = fig.add_subplot(111)
        ax.plot(Prediction.index, Prediction, "darkblue", linewidth=1.0)
        ax.scatter(Well_set_original.index, Well_set_original, color='darkorange', marker='*', s=10)
        ax.set_ylabel('Groundwater Level')
        ax.legend(['Prediction', 'Training Data', 'Test Data'])
        ax.set_title('Observation Vs Prediction: ' + name)
        if error_on:
          ax.text(x=0.0, y=-0.15, s = metrics[['Train ME','Train RMSE', 'Train MAE', 'Train r2']].to_string(index=True, float_format = "{0:.3}".format),
                  fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)
          ax.text(x=0.25, y=-0.15, s = metrics[['Validation ME','Validation RMSE', 'Validation MAE', 'Validation r2']].to_string(index=True, float_format = "{0:.3}".format),
                  fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)  
          ax.text(x=0.5, y=-0.15, s = metrics[['Test ME','Test RMSE', 'Test MAE', 'Test r2']].to_string(index=True, float_format = "{0:.3}".format),
                  fontsize = 12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)
          extent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())
          fig.savefig(self.figures_root  + '/' + name + '_08_Test_kfold', bbox_inches=extent.expanded(1.2, 1.6))
        else: fig.savefig(self.figures_root  + '/' + name + '_08_Test_kfold')
        if show: plt.show()
        else:
            fig.clf()
            plt.close(fig)    
        gc.collect()
        
    def Feature_Importance_box_plot(self, importance_df, show=False):
        #All Data       
        importance_df.boxplot(figsize=(20,10))
        plt.xlabel('Feature')
        plt.ylabel('Relative Importance')
        plt.xticks(rotation=90)
        plt.tight_layout()
        plt.savefig(self.figures_root  + '/' + 'Feature_Importance_Complete')
        if show: plt.show()
        else: plt.close()
    
        #Calc Mean and sort
        importance_mean_df = importance_df.mean()
        importance_mean_df = pd.DataFrame(importance_mean_df.sort_values(axis=0, ascending=False)).T
        importance_mean = importance_df.transpose().reindex(list(importance_mean_df.columns)).transpose()
        importance_mean.iloc[:,:10].boxplot(figsize=(5,5))
        plt.xticks(rotation=90)
        plt.xlabel('Feature')
        plt.ylabel('Relative Importance')
        plt.title('Most Prevalent Features:')
        plt.tight_layout()
        plt.savefig(self.figures_root  + '/' + 'Feature_Importance_Uppper')
        if show: plt.show()
        else: plt.close()
        
        #Lower
        importance_mean.iloc[:,importance_mean.shape[1]-10:].boxplot(figsize=(5,5))
        plt.xticks(rotation=90)
        plt.xlabel('Feature')
        plt.ylabel('Relative Importance')
        plt.title('Least Prevalent Features:')
        plt.tight_layout()
        plt.savefig(self.figures_root  + '/' + 'Feature_Importance_Lower')
        if show: plt.show()
        else: plt.close()
        gc.collect()

    def feature_plot(self, Feature_Data, raw, name, show=False):
        fig = plt.figure(figsize=(6, 4))
        ax = fig.add_subplot(111)
        ax.plot(Feature_Data)
        for i, n in enumerate(Feature_Data.columns[1:]):
            ax.scatter(raw.index, raw[n], color='black', marker='*', s=3)
        legend = Feature_Data.columns.tolist() + ['Observed Measurements']
        ax.legend(legend, loc="lower left", bbox_to_anchor=(0.02, -0.35),
          ncol=2, fancybox=True, shadow=True)
        ax.set_ylabel('Groundwater Level')
        ax.set_title('Well Feature Correlation: ' + name)
        extent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())
        plt.savefig(self.figures_root  + '/' + name + '_09_Features', bbox_inches=extent.expanded(1.3, 1.8))
        if show: plt.show()
        else:
            fig.clf()
            plt.close(fig)
        gc.collect()

    def Aquifer_Plot(self, imputed_df, show=False):
        fig = plt.figure(figsize=(12, 8))
        plt.plot(imputed_df)
        plt.title('Measured and Interpolated data for all wells')
        plt.savefig(self.figures_root  + '/' + 'Aquifer_Plot')
        if show: plt.show()
        else:
            fig.clf()
            plt.close(fig)
        gc.collect()
           