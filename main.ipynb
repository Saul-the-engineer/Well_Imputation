{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import utils_preprocess\n",
    "import utils_spatial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Preprocessing PDSI and GLDAS data into Tabular Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the pdsi netcdf files to obtain tabular data pickle file\n",
    "pdsi_source_directory = r'C:\\Users\\saulg\\Desktop\\Remote_Data\\pdsi'\n",
    "pdsi_target_directory = r'C:\\Users\\saulg\\Desktop\\Remote_Data\\pdsi_tabular'\n",
    "\n",
    "utils_preprocess.process_pdsi_data(source_directory=pdsi_source_directory, target_directory=pdsi_target_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the gldas netcdf files to obtain tabular data pickle file\n",
    "gldas_source_directory = r'C:\\Users\\saulg\\Desktop\\Remote_Data\\GLDAS'\n",
    "gldas_target_directory = r'C:\\Users\\saulg\\Desktop\\Remote_Data\\gldas_tabular'\n",
    "\n",
    "utils_preprocess.process_gldas_data(source_directory=gldas_source_directory, target_directory=gldas_target_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Transform PDSI, GLDAS, and Well Observations into format for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile\n",
    "path_shape = '/home/saul/workspace/Well_Imputation/Aquifer Shapes/Beryl_Enterprise.shp'\n",
    "aquifer_shape = utils.load_shapefile(path_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse pdsi data and save it\n",
    "directory_pdsi = r\"/mnt/c/Users/saulg/Desktop/Remote_Data/pdsi_tabular\"\n",
    "pdsi = utils.pull_relevant_data(aquifer_shape, dataset_name=\"PDSI\", dataset_directory=directory_pdsi)\n",
    "utils.save_pickle(pdsi, \"pdsi_data.pickle\", \"Datasets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the GLDAS data and save it\n",
    "directory_gldas = r\"/mnt/c/Users/saulg/Desktop/Remote_Data/gldas_tabular\"\n",
    "gldas = utils.pull_relevant_data(aquifer_shape, dataset_name=\"GLDAS\", dataset_directory=directory_gldas)\n",
    "utils.save_pickle(gldas, \"gldas_data.pickle\", \"Datasets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process well data from csv files\n",
    "well_locations = pd.read_csv(\"Aquifers Data/EscalanteBerylLocation.csv\")\n",
    "well_timeseries = pd.read_csv(\"Aquifers Data/EscalanteBerylTimeseries.csv\")\n",
    "data = utils.transform_well_data(well_timeseries, well_locations)\n",
    "utils.save_pickle(data, \"BerylEnterpriseData.pickle\", \"./Datasets/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the timeseries data to see if it looks reasonable\n",
    "plt.plot(data[\"timeseries\"], '-.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Develop initial imputation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Develop iterative refinement model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Analyze spatial characteristics of imputation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = utils.load_pickle(\"Datasets/Well_Data_Imputed_iteration_2.pickle\")\n",
    "\n",
    "utils_spatial.krigging_interpolation(data = data,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Calculate Storage Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster = nc.Dataset(\"Datasets/well_data_iter_2.nc\", 'r')\n",
    "out = utils_spatial.StorageChangeCalculator()\n",
    "ts = out.calulate_storage_curve(raster, date_range_filter=(\"1948-01-01\", \"2020-12-31\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Geospatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
